# SEQUENCER: Sequence-to-Sequence Learning for End-to-End Program Repair

## Abstract

This paper presents a novel end-to-end approach to program repair based on sequence-to-sequence learning. We devise, implement, and evaluate a technique, called SEQUENCER, for fixing bugs based on sequence-to-sequence learning on source code. This approach uses the copy mechanism to overcome the unlimited vocabulary problem that occurs with big code. Our system is data-driven; we train it on 35,578 samples, carefully curated from commits to open-source repositories. We evaluate SEQUENCER on 4,711 independent real bug fixes, as well on the Defects4J benchmark used in program repair research. SEQUENCER is able to perfectly predict the fixed line for 950/4,711 testing samples, and find correct patches for 14 bugs in Defects4J benchmark. SEQUENCER captures a wide range of repair operators without any domain-specific top-down design.

æå‡ºäº†ä¸€ç§åŸºäºåºåˆ—-åºåˆ—å­¦ä¹ çš„ç«¯åˆ°ç«¯ç¨‹åºä¿®å¤æ–¹æ³•ã€‚æˆ‘ä»¬è®¾è®¡ã€å®ç°å¹¶è¯„ä¼°äº†ä¸€ç§ç§°ä¸ºâ€œæ’åºå™¨â€çš„æŠ€æœ¯ï¼Œè¯¥æŠ€æœ¯åŸºäºå¯¹æºä»£ç çš„é¡ºåºåˆ°é¡ºåºçš„å­¦ä¹ æ¥ä¿®å¤bugã€‚è¿™ç§æ–¹æ³•ä½¿ç”¨å¤åˆ¶æœºåˆ¶æ¥å…‹æœå¤§ä»£ç ä¸­å‡ºç°çš„æ— é™è¯æ±‡è¡¨é—®é¢˜ã€‚æˆ‘ä»¬çš„ç³»ç»Ÿæ˜¯æ•°æ®é©±åŠ¨çš„;æˆ‘ä»¬å¯¹35,578ä¸ªæ ·æœ¬è¿›è¡Œäº†åŸ¹è®­ï¼Œè¿™äº›æ ·æœ¬éƒ½æ˜¯ä»æäº¤åˆ°å¼€æºåº“ä¸­ç²¾å¿ƒæŒ‘é€‰å‡ºæ¥çš„ã€‚æˆ‘ä»¬åœ¨4711ä¸ªç‹¬ç«‹çš„å®é™…bugä¿®å¤ä¸Šä»¥åŠåœ¨ç¨‹åºä¿®å¤ç ”ç©¶ä¸­ä½¿ç”¨çš„ç¼ºé™·4jåŸºå‡†ä¸Šè¯„ä¼°äº†éŸ³åºå™¨ã€‚sequalizerèƒ½å¤Ÿå®Œç¾åœ°é¢„æµ‹950/ 4711æµ‹è¯•æ ·æœ¬çš„å›ºå®šè¡Œæ•°ï¼Œå¹¶ä¸ºæ’é”™æµ‹è¯•ä¸­çš„14ä¸ªbugæ‰¾åˆ°æ­£ç¡®çš„è¡¥ä¸ã€‚éŸ³åºå™¨æ— éœ€ä»»ä½•é¢†åŸŸç‰¹å®šçš„è‡ªé¡¶å‘ä¸‹è®¾è®¡ï¼Œå°±å¯ä»¥æ•è·å¹¿æ³›çš„ä¿®å¤æ“ä½œç¬¦ã€‚

## 1 INTRODUCTION

PEOPLE have long dreamed of machines capable of writing computer programs by themselves. Having machines writing a full software system is science-fiction but teaching machines to modify an existing program to fix a bug is within the reach of current software technology; this is called automated program repair [1].

äººä»¬ä¸€ç›´æ¢¦æƒ³ç€èƒ½è‡ªå·±ç¼–å†™è®¡ç®—æœºç¨‹åºçš„æœºå™¨ã€‚è®©æœºå™¨ç¼–å†™å®Œæ•´çš„è½¯ä»¶ç³»ç»Ÿæ˜¯ç§‘å¹»å°è¯´ï¼Œä½†æ•™æœºå™¨ä¿®æ”¹ç°æœ‰ç¨‹åºä»¥ä¿®å¤bugæ˜¯å½“å‰è½¯ä»¶æŠ€æœ¯æ‰€èƒ½åšåˆ°çš„;è¿™å°±æ˜¯æ‰€è°“çš„è‡ªåŠ¨ç¨‹åºä¿®å¤[1]ã€‚

Program repair research is very active and dominated by techniques based on static analysis (e.g., Angelix [2]) and dynamic analysis (e.g., CapGen [3]). While great progress has been achieved, the current state of automated program repair is limited to simple small fixes, mostly one line patches [3], [4]. These techniques are heavily top-down, based on intelligent design and domain-specific knowledge about bug fixing in a given language or a specific application domain. In this paper, we also focus on one line patches, but we aim at doing program repair in a language agnostic generic manner, fully relying on machine learning to capture syntax and grammar rules and produce wellformed, compilable programs. By taking this approach, we aim to provide a foundation for connecting program repair and machine learning, allowing the program repair community to benefit from training with more complete bug datasets and continued improvements to machine learning algorithms and libraries.

ç¨‹åºä¿®å¤ç ”ç©¶éå¸¸æ´»è·ƒï¼Œå¹¶ä»¥åŸºäºé™æ€åˆ†æ(ä¾‹å¦‚ï¼ŒAngelix[2])å’ŒåŠ¨æ€åˆ†æ(ä¾‹å¦‚ï¼ŒCapGen[3])çš„æŠ€æœ¯ä¸ºä¸»å¯¼ã€‚è™½ç„¶å·²ç»å–å¾—äº†å¾ˆå¤§çš„è¿›å±•ï¼Œä½†ç›®å‰çš„è‡ªåŠ¨ç¨‹åºä¿®å¤çŠ¶æ€ä»…é™äºç®€å•çš„å°ä¿®å¤ï¼Œä¸»è¦æ˜¯ä¸€è¡Œè¡¥ä¸[3]ã€[4]ã€‚è¿™äº›æŠ€æœ¯å¾ˆå¤§ç¨‹åº¦ä¸Šæ˜¯è‡ªé¡¶å‘ä¸‹çš„ï¼ŒåŸºäºæ™ºèƒ½è®¾è®¡å’Œç‰¹å®šé¢†åŸŸçš„çŸ¥è¯†ï¼Œåœ¨ç»™å®šçš„è¯­è¨€æˆ–ç‰¹å®šçš„åº”ç”¨ç¨‹åºé¢†åŸŸä¸­ä¿®å¤é”™è¯¯ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä¹Ÿå…³æ³¨å•è¡Œè¡¥ä¸ï¼Œä½†æˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä»¥ä¸€ç§è¯­è¨€æ— å…³çš„é€šç”¨æ–¹å¼è¿›è¡Œç¨‹åºä¿®å¤ï¼Œå®Œå…¨ä¾é æœºå™¨å­¦ä¹ æ¥æ•è·è¯­æ³•å’Œè¯­æ³•è§„åˆ™ï¼Œå¹¶ç”Ÿæˆæ ¼å¼è‰¯å¥½çš„å¯ç¼–è¯‘ç¨‹åºã€‚é€šè¿‡é‡‡ç”¨è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬çš„ç›®æ ‡æ˜¯ä¸ºç¨‹åºä¿®å¤å’Œæœºå™¨å­¦ä¹ ä¹‹é—´çš„è¿æ¥æä¾›ä¸€ä¸ªåŸºç¡€ï¼Œä½¿ç¨‹åºä¿®å¤ç¤¾åŒºèƒ½å¤Ÿå—ç›Šäºä½¿ç”¨æ›´å®Œæ•´çš„é”™è¯¯æ•°æ®é›†è¿›è¡Œçš„åŸ¹è®­ï¼Œä»¥åŠå¯¹æœºå™¨å­¦ä¹ ç®—æ³•å’Œåº“çš„æŒç»­æ”¹è¿›ã€‚

As the foundation for our model, we apply sequence-to-sequence learning [5] to the problem of program repair. Sequence-to-sequence learning is a branch of statistical ma-chine learning, mostly used for machine translation: the algorithm learns to translate text from one language (say French) to another language (say Swedish) by generalizing over large amounts of sentence pairs from French to Swedish. The training data comes from the large amount of text already translated by humans, starting with the Rosetta stone written in 196 BC [6]. The name of the technique is explicit: it is about learning to translate from one sequence of words to another sequence of words.

ä½œä¸ºæ¨¡å‹çš„åŸºç¡€ï¼Œæˆ‘ä»¬å°†åºåˆ—-åºåˆ—å­¦ä¹ [5]åº”ç”¨äºç¨‹åºä¿®å¤é—®é¢˜ã€‚åºåˆ—-åºåˆ—å­¦ä¹ æ˜¯ç»Ÿè®¡æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸»è¦ç”¨äºæœºå™¨ç¿»è¯‘:è¯¥ç®—æ³•é€šè¿‡å°†å¤§é‡çš„å¥å­å¯¹ä»æ³•è¯­åˆ°ç‘å…¸è¯­è¿›è¡Œå½’çº³ï¼Œå­¦ä¹ å¦‚ä½•å°†æ–‡æœ¬ä»ä¸€ç§è¯­è¨€(å¦‚æ³•è¯­)ç¿»è¯‘æˆå¦ä¸€ç§è¯­è¨€(å¦‚ç‘å…¸è¯­)ã€‚è®­ç»ƒæ•°æ®æ¥è‡ªå¤§é‡å·²ç»ç”±äººç±»ç¿»è¯‘çš„æ–‡æœ¬ï¼Œä»å…¬å…ƒå‰196å¹´çš„ç½—å¡å¡”çŸ³ç¢‘å¼€å§‹ã€‚è¿™é¡¹æŠ€æœ¯çš„åç§°å¾ˆæ˜ç¡®:å®ƒæ˜¯å…³äºå­¦ä¹ å°†ä¸€ä¸ªå•è¯åºåˆ—ç¿»è¯‘æˆå¦ä¸€ä¸ªå•è¯åºåˆ—ã€‚

Now let us come back to the problem of programming: we want to learn to â€™translateâ€™ from one sequence of program tokens (a buggy program) to a different sequence of program tokens (a fixed program). The training data is readily available: we have millions of commits in opensource code repositories. Yet, we still have major challenges to overcome when it comes to using sequence-to-sequence learning on code: 1) the raw (unfiltered) data is rather noisy; one must deploy significant effort to identify and curate commits that focus on a clear task; 2) contrary to natural language, misuse of rare words (identifiers, numbers, etc) is often fatal in programming languages [7]; in natural language some errors may be tolerable because of the intelligence of the human reader while in programming languages the compiler (or interpreter) is strict 3) in natural language, the dependencies are often in the same sentence (â€œitâ€ refers to â€œdogâ€ just before) , or within a couple of sentences, while in programming, the dependencies have a longer range: one may use a variable that has been declared dozens of lines before.

ç°åœ¨è®©æˆ‘ä»¬å›åˆ°ç¼–ç¨‹é—®é¢˜ä¸Š:æˆ‘ä»¬æƒ³è¦å­¦ä¹ å¦‚ä½•å°†ä¸€ä¸ªç¨‹åºæ ‡è®°åºåˆ—(ä¸€ä¸ªæœ‰bugçš„ç¨‹åº)â€œè½¬æ¢â€ä¸ºå¦ä¸€ä¸ªç¨‹åºæ ‡è®°åºåˆ—(ä¸€ä¸ªå›ºå®šçš„ç¨‹åº)ã€‚åŸ¹è®­æ•°æ®éšæ—¶å¯ç”¨:æˆ‘ä»¬åœ¨å¼€æ”¾æºä»£ç åº“ä¸­æœ‰æ•°ç™¾ä¸‡æ¬¡æäº¤ã€‚ç„¶è€Œï¼Œåœ¨å¯¹ä»£ç ä½¿ç”¨é¡ºåºåˆ°é¡ºåºçš„å­¦ä¹ æ—¶ï¼Œæˆ‘ä»¬ä»ç„¶æœ‰ä¸€äº›ä¸»è¦çš„æŒ‘æˆ˜éœ€è¦å…‹æœ:1)åŸå§‹(æœªç»è¿‡æ»¤çš„)æ•°æ®ç›¸å½“å˜ˆæ‚;ä¸€ä¸ªäººå¿…é¡»éƒ¨ç½²å¤§é‡çš„å·¥ä½œæ¥è¯†åˆ«å’Œç®¡ç†é›†ä¸­åœ¨ä¸€ä¸ªæ˜ç¡®ä»»åŠ¡ä¸Šçš„æäº¤;2)ä¸è‡ªç„¶è¯­è¨€ç›¸åï¼Œåœ¨[7]ç¼–ç¨‹è¯­è¨€ä¸­ï¼Œç½•è§è¯(æ ‡è¯†ç¬¦ã€æ•°å­—ç­‰)çš„è¯¯ç”¨å¾€å¾€æ˜¯è‡´å‘½çš„;åœ¨è‡ªç„¶è¯­è¨€,å› ä¸ºä¸€äº›é”™è¯¯å¯ä»¥æ¥å—äººç±»è¯»è€…çš„æƒ…æŠ¥åœ¨ç¼–ç¨‹è¯­è¨€ç¼–è¯‘å™¨(æˆ–ç¿»è¯‘)æ˜¯ä¸¥æ ¼3)åœ¨è‡ªç„¶è¯­è¨€ä¸­,ä¾èµ–å…³ç³»é€šå¸¸æ˜¯åœ¨åŒä¸€ä¸ªå¥å­(â€œå®ƒâ€æŒ‡çš„æ˜¯â€œç‹—â€ä¹‹å‰),æˆ–åœ¨å‡ ä¸ªå¥å­,åœ¨ç¼–ç¨‹ä¸­,ä¾èµ–æœ‰æ›´é•¿æ—¶é—´èŒƒå›´:ä¸€ä¸ªå¯èƒ½ä½¿ç”¨ä¸€ä¸ªå˜é‡è¢«å®£å‘Šæ•°åè¡Œä¹‹å‰ã€‚

We are now at a tipping point to address those challenges. First, sequence-to-sequence learning has reached a maturity level, both conceptually and from an implementation point of view, that it can be fed with sequences whose characteristics significantly differ from natural language. Second, there has been great recent progress on using various types of language models on source code [8]. Based on this great body of work, we present our approach to using sequence-to-learning for program repair, which we created to repair real bugs from large open-source projects written in the Java programming language.

æˆ‘ä»¬ç°åœ¨æ­£å¤„äºåº”å¯¹è¿™äº›æŒ‘æˆ˜çš„è½¬æŠ˜ç‚¹ã€‚é¦–å…ˆï¼Œä»æ¦‚å¿µå’Œå®ç°çš„è§’åº¦æ¥çœ‹ï¼Œåºåˆ—åˆ°åºåˆ—çš„å­¦ä¹ å·²ç»è¾¾åˆ°äº†ä¸€ä¸ªæˆç†Ÿçš„æ°´å¹³ï¼Œå®ƒå¯ä»¥ç”±ä¸è‡ªç„¶è¯­è¨€æ˜¾è‘—ä¸åŒçš„ç‰¹æ€§ç»„æˆçš„åºåˆ—æ¥æä¾›ã€‚å…¶æ¬¡ï¼Œæœ€è¿‘åœ¨æºä»£ç [8]ä¸Šä½¿ç”¨å„ç§ç±»å‹çš„è¯­è¨€æ¨¡å‹æ–¹é¢å–å¾—äº†å¾ˆå¤§çš„è¿›å±•ã€‚åŸºäºè¿™äº›å¤§é‡çš„å·¥ä½œï¼Œæˆ‘ä»¬æå‡ºäº†ä½¿ç”¨é¡ºåºå­¦ä¹ è¿›è¡Œç¨‹åºä¿®å¤çš„æ–¹æ³•ï¼Œæˆ‘ä»¬åˆ›å»ºè¿™äº›æ–¹æ³•æ˜¯ä¸ºäº†ä¿®å¤ç”¨Javaç¼–ç¨‹è¯­è¨€ç¼–å†™çš„å¤§å‹å¼€æºé¡¹ç›®ä¸­çš„å®é™…bugã€‚

Our end-to-end program repair approach is called SEQUENCER and it works as follows. First, we focus on oneline fixes: we predict the fixed version of a buggy programming line. For this, we create a carefully curated training and testing dataset of one-line commits. Second, we devise a sequence-to-sequence network architecture that is specifically designed to address the two main aforementioned challenges. To address the unlimited vocabulary problem, we use the copy mechanism [9]; this allows SEQUENCER to predict the fixed line, even if the fix contains a token that was too rare (i.e., an API call that appears only in few cases, or a rare identifier used only in one class) to be considered in the vocabulary. This copy mechanism works even if the fixed line should contain tokens which were not in the training set. To address the dependency problem, we construct abstract buggy context from the buggy class, which captures the most important context around the buggy source code and reduces the complexity of the input sequence. This enables us to capture long range dependencies that are required for the fix.

æˆ‘ä»¬çš„ç«¯åˆ°ç«¯ç¨‹åºä¿®å¤æ–¹æ³•ç§°ä¸ºåºåˆ—å™¨ï¼Œå®ƒçš„å·¥ä½œåŸç†å¦‚ä¸‹ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬å…³æ³¨ä¸€è¡Œä¿®å¤:æˆ‘ä»¬é¢„æµ‹æœ‰bugçš„ç¼–ç¨‹è¡Œçš„å›ºå®šç‰ˆæœ¬ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ªç²¾å¿ƒç­–åˆ’çš„å•è¡Œæäº¤åŸ¹è®­å’Œæµ‹è¯•æ•°æ®é›†ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ä¸ªé¡ºåºåˆ°é¡ºåºçš„ç½‘ç»œæ¶æ„ï¼Œä¸“é—¨ç”¨äºè§£å†³å‰é¢æåˆ°çš„ä¸¤ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚ä¸ºäº†è§£å†³æ— é™è¯æ±‡é‡çš„é—®é¢˜ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å¤åˆ¶æœºåˆ¶[9];è¿™ä½¿å¾—éŸ³åºå™¨èƒ½å¤Ÿé¢„æµ‹å›ºå®šçš„è¡Œï¼Œå³ä½¿ä¿®æ­£åŒ…å«äº†ä¸€ä¸ªéå¸¸ç½•è§çš„æ ‡è®°(ä¾‹å¦‚ã€‚åœ¨è¯æ±‡è¡¨ä¸­è€ƒè™‘çš„APIè°ƒç”¨(ä»…åœ¨å°‘æ•°æƒ…å†µä¸‹å‡ºç°ï¼Œæˆ–ä»…åœ¨ä¸€ä¸ªç±»ä¸­ä½¿ç”¨çš„ç½•è§æ ‡è¯†ç¬¦)ã€‚è¿™ä¸ªå‰¯æœ¬æœºåˆ¶å³ä½¿å›ºå®šçº¿åº”è¯¥åŒ…å«ä»¤ç‰Œæ²¡æœ‰åœ¨è®­ç»ƒé›†,è§£å†³ä¾èµ–æ€§é—®é¢˜,æˆ‘ä»¬æ„å»ºæŠ½è±¡çš„è½¦ä»è½¦ç±»ä¸Šä¸‹æ–‡,å®ƒæ•æ‰æœ€é‡è¦çš„ä¸Šä¸‹æ–‡åœ¨è½¦æºä»£ç å’Œå‡å°‘è¾“å…¥åºåˆ—çš„å¤æ‚æ€§ã€‚è¿™ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ•è·ä¿®å¤æ‰€éœ€çš„é•¿æœŸä¾èµ–é¡¹ã€‚

We evaluate SEQUENCER in two ways. First, we compute accuracy over 4,711 real one-line commits, curated from three open-source projects. The accuracy is measured by the ability of the system to predict the fixed line exactly as originally crafted by the developer, given as input the buggy file and the buggy line number. Our golden configuration is able to perfectly predict the fix for 950/4,711 (20%) of the testing samples. This sets up a baseline for future research in the field. Second, we apply SEQUENCER to the mainstream evaluation benchmark for program repair, Defects4J. Of the 395 total bugs in Defects4J, 75 have one-line replacement repairs; SEQUENCER generates patches which pass the test suite for 19 bugs and patches which are semantically equivalent to the human-generated patch for 14 bugs. To our knowledge, this is the first report ever on using sequence-to-sequence learning for end-to-end program repair, including validation with test cases.

æˆ‘ä»¬è¯„ä¼°éŸ³åºå™¨æœ‰ä¸¤ç§æ–¹å¼ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¡ç®—å‡ºè¶…è¿‡4711ä¸ªçœŸæ­£çš„å•è¡Œæäº¤çš„å‡†ç¡®æ€§ï¼Œè¿™æ˜¯ç”±ä¸‰ä¸ªå¼€æºé¡¹ç›®ç­–åˆ’çš„ã€‚å‡†ç¡®æ€§æ˜¯é€šè¿‡ç³»ç»Ÿé¢„æµ‹å›ºå®šè¡Œçš„èƒ½åŠ›æ¥è¡¡é‡çš„ï¼Œå®ƒä¸æœ€åˆç”±å¼€å‘äººå‘˜è®¾è®¡çš„å®Œå…¨ä¸€æ ·ï¼Œè¾“å…¥é”™è¯¯æ–‡ä»¶å’Œé”™è¯¯è¡Œå·ã€‚æˆ‘ä»¬çš„é»„é‡‘é…ç½®èƒ½å¤Ÿå®Œç¾åœ°é¢„æµ‹950/ 4711(20%)æµ‹è¯•æ ·æœ¬çš„ä¿®å¤ã€‚è¿™ä¸ºè¯¥é¢†åŸŸæœªæ¥çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†éŸ³åºå™¨åº”ç”¨äºç¨‹åºä¿®å¤çš„ä¸»æµè¯„ä¼°åŸºå‡†â€”â€”ç¼ºé™·(defect)ã€‚åœ¨defect ts4jçš„395ä¸ªbugä¸­ï¼Œæœ‰75ä¸ªæ˜¯å•è¡Œæ›¿æ¢ä¿®å¤;éŸ³åºå™¨ç”Ÿæˆçš„è¡¥ä¸é€šè¿‡äº†19ä¸ªbugçš„æµ‹è¯•å¥—ä»¶ï¼Œè€Œè¿™äº›è¡¥ä¸åœ¨è¯­ä¹‰ä¸Šç›¸å½“äºäººä¸ºç”Ÿæˆçš„14ä¸ªbugçš„è¡¥ä¸ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢å…³äºä½¿ç”¨åºåˆ—åˆ°åºåˆ—å­¦ä¹ è¿›è¡Œç«¯åˆ°ç«¯ç¨‹åºä¿®å¤(åŒ…æ‹¬ä½¿ç”¨æµ‹è¯•ç”¨ä¾‹è¿›è¡ŒéªŒè¯)çš„ç¬¬ä¸€ä¸ªæŠ¥å‘Šã€‚

Overall, the novelty of this work is as follows. First, we create and share a unique dataset for evaluating learning techniques on one-line program repair. Second, we report on using the copy mechanism on seq-to-seq learning on source code. Third, on the same buggy input dataset, SEQUENCER is able to produce the correct patch for 119% more samples than the closest related work [10].

æˆ‘ä»¬è¯„ä¼°éŸ³åºå™¨æœ‰ä¸¤ç§æ–¹å¼ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬è®¡ç®—å‡ºè¶…è¿‡4711ä¸ªçœŸæ­£çš„å•è¡Œæäº¤çš„å‡†ç¡®æ€§ï¼Œè¿™æ˜¯ç”±ä¸‰ä¸ªå¼€æºé¡¹ç›®ç­–åˆ’çš„ã€‚å‡†ç¡®æ€§æ˜¯é€šè¿‡ç³»ç»Ÿé¢„æµ‹å›ºå®šè¡Œçš„èƒ½åŠ›æ¥è¡¡é‡çš„ï¼Œå®ƒä¸æœ€åˆç”±å¼€å‘äººå‘˜è®¾è®¡çš„å®Œå…¨ä¸€æ ·ï¼Œè¾“å…¥é”™è¯¯æ–‡ä»¶å’Œé”™è¯¯è¡Œå·ã€‚æˆ‘ä»¬çš„é»„é‡‘é…ç½®èƒ½å¤Ÿå®Œç¾åœ°é¢„æµ‹950/ 4711(20%)æµ‹è¯•æ ·æœ¬çš„ä¿®å¤ã€‚è¿™ä¸ºè¯¥é¢†åŸŸæœªæ¥çš„ç ”ç©¶å¥ å®šäº†åŸºç¡€ã€‚å…¶æ¬¡ï¼Œæˆ‘ä»¬å°†éŸ³åºå™¨åº”ç”¨äºç¨‹åºä¿®å¤çš„ä¸»æµè¯„ä¼°åŸºå‡†â€”â€”ç¼ºé™·(defect)ã€‚åœ¨defect ts4jçš„395ä¸ªbugä¸­ï¼Œæœ‰75ä¸ªæ˜¯å•è¡Œæ›¿æ¢ä¿®å¤;éŸ³åºå™¨ç”Ÿæˆçš„è¡¥ä¸é€šè¿‡äº†19ä¸ªbugçš„æµ‹è¯•å¥—ä»¶ï¼Œè€Œè¿™äº›è¡¥ä¸åœ¨è¯­ä¹‰ä¸Šç›¸å½“äºäººä¸ºç”Ÿæˆçš„14ä¸ªbugçš„è¡¥ä¸ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯è¿„ä»Šä¸ºæ­¢å…³äºä½¿ç”¨åºåˆ—åˆ°åºåˆ—å­¦ä¹ è¿›è¡Œç«¯åˆ°ç«¯ç¨‹åºä¿®å¤(åŒ…æ‹¬ä½¿ç”¨æµ‹è¯•ç”¨ä¾‹è¿›è¡ŒéªŒè¯)çš„ç¬¬ä¸€ä¸ªæŠ¥å‘Šã€‚

To sum up:  
- Our key contribution is an approach for fixing bugs based on sequence-to-sequence learning on token sequences. This approach uses the copy mechanism to overcome the unlimited vocabulary problem in source code.  
- We present the construction of an abstract buggy context that leverages code context for patch generation. The input program token sequences are at the level of full classes and capture long-range dependencies in the fix to be written.We implement our approach in a publiclyavailable program repair tool called SEQUENCER.  
- We evaluate our approach on 4,711 real bug fixing tasks. Contrary to the closest related work [10], we do not assume bugs to be in small methods only. Our golden trained model is able to perfectly fix 950/4,711 testing samples. To the best-of-our knowledge, this is the best result reported on such a task at the time of writing this paper [10][11][12].  
- We evaluate our approach on the 75 one-line bugs of Defects4J, which is the most widely used benchmark for evaluating programming repair contributions. SEQUENCER is able to find 2,321 patches for these bugs, 761 compile successfully, 61 are plausible (they pass the full test suite) and 18 are semantically equivalent to the patch written by the human developer.  
- We provide a qualitative analysis of 8 interesting repair operators captured by sequence-to-sequence learning on the considered training dataset.

æ€»ç»“:
- æˆ‘ä»¬çš„ä¸»è¦è´¡çŒ®æ˜¯ä¸€ä¸ªåŸºäºä»¤ç‰Œåºåˆ—çš„åºåˆ—åˆ°åºåˆ—çš„å­¦ä¹ æ¥ä¿®å¤bugçš„æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•ä½¿ç”¨å¤åˆ¶æœºåˆ¶æ¥å…‹æœæºä»£ç ä¸­çš„æ— é™è¯æ±‡è¡¨é—®é¢˜ã€‚
- æˆ‘ä»¬æå‡ºäº†ä¸€ä¸ªæŠ½è±¡çš„æœ‰bugçš„ä¸Šä¸‹æ–‡ç»“æ„ï¼Œå®ƒåˆ©ç”¨ä»£ç ä¸Šä¸‹æ–‡æ¥ç”Ÿæˆè¡¥ä¸ã€‚è¾“å…¥ç¨‹åºä»¤ç‰Œåºåˆ—å¤„äºå®Œæ•´ç±»çš„çº§åˆ«ï¼Œå¹¶åœ¨è¦ç¼–å†™çš„ä¿®å¤ç¨‹åºä¸­æ•è·é•¿æœŸä¾èµ–é¡¹ã€‚æˆ‘ä»¬åœ¨ä¸€ä¸ªå…¬å¼€å¯ç”¨çš„ç¨‹åºä¿®å¤å·¥å…·ä¸­å®ç°äº†æˆ‘ä»¬çš„æ–¹æ³•ï¼Œè¿™ä¸ªå·¥å…·å«åšéŸ³åºå™¨ã€‚
- æˆ‘ä»¬è¯„ä¼°äº†4711ä¸ªçœŸæ­£çš„bugä¿®å¤ä»»åŠ¡ã€‚ä¸æœ€è¿‘çš„ç›¸å…³å·¥ä½œ[10]ç›¸åï¼Œæˆ‘ä»¬ä¸å‡è®¾bugåªæ˜¯åœ¨å°æ–¹æ³•ä¸­ã€‚æˆ‘ä»¬çš„é»„é‡‘è®­ç»ƒæ¨¡å‹èƒ½å¤Ÿå®Œç¾åœ°ä¿®å¤950/ 4711æµ‹è¯•æ ·æœ¬ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¿™æ˜¯åœ¨æ’°å†™è¿™ç¯‡è®ºæ–‡æ—¶å…³äºè¿™ç±»ä»»åŠ¡æ‰€æŠ¥å‘Šçš„æœ€ä½³ç»“æœã€‚
- æˆ‘ä»¬è¯„ä¼°äº†æˆ‘ä»¬çš„æ–¹æ³•åœ¨25è¡Œç¼ºé™·ä¸­çš„ç¼ºé™·ï¼Œè¿™æ˜¯æœ€å¹¿æ³›ä½¿ç”¨çš„è¯„ä¼°ç¼–ç¨‹ä¿®å¤è´¡çŒ®çš„åŸºå‡†ã€‚éŸ³åºå™¨èƒ½å¤Ÿæ‰¾åˆ°2,321ä¸ªè¡¥ä¸ï¼Œ761ä¸ªç¼–è¯‘æˆåŠŸï¼Œ61ä¸ªæ˜¯å¯ä¿¡çš„(å®ƒä»¬é€šè¿‡äº†å®Œæ•´çš„æµ‹è¯•å¥—ä»¶)ï¼Œ18ä¸ªåœ¨è¯­ä¹‰ä¸Šç­‰åŒäºäººç±»å¼€å‘äººå‘˜ç¼–å†™çš„è¡¥ä¸ã€‚
- æˆ‘ä»¬æä¾›äº†ä¸€ä¸ªå®šæ€§çš„åˆ†æï¼Œ8ä¸ªæœ‰è¶£çš„ç»´ä¿®æ“ä½œå‘˜æ•è·çš„åºåˆ—åˆ°åºåˆ—çš„å­¦ä¹ å¯¹è€ƒè™‘çš„è®­ç»ƒæ•°æ®é›†ã€‚

## 2 BACKGROUND ON NEURAL MACHINE TRANSLATION
WITH SEQUENCE-TO-SEQUENCE LEARNING

SEQUENCER is based on the idea of receiving buggy code as input and producing fixed code as output. The concept is similar to neural machine translation where the input is a sequence of words in one language and the output is a sequence in another language. In this section, we provide a brief introduction to neural machine translation (NMT).

éŸ³åºå™¨æ˜¯åŸºäºæ¥æ”¶é”™è¯¯ä»£ç ä½œä¸ºè¾“å…¥ï¼Œäº§ç”Ÿå›ºå®šä»£ç ä½œä¸ºè¾“å‡ºçš„æ€æƒ³ã€‚è¿™ä¸ªæ¦‚å¿µç±»ä¼¼äºç¥ç»æœºå™¨ç¿»è¯‘ï¼Œå³è¾“å…¥æ˜¯ä¸€ç§è¯­è¨€çš„å•è¯åºåˆ—ï¼Œè¾“å‡ºæ˜¯å¦ä¸€ç§è¯­è¨€çš„å•è¯åºåˆ—ã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ç®€è¦ä»‹ç»ç¥ç»æœºå™¨ç¿»è¯‘(NMT)ã€‚

In neural machine translation, the dominant technique is called â€œsequence-to-sequence learningâ€, where â€œsequenceâ€ refers to the sequence of words in a sentence. An early example of a sequence-to-sequence network [5] used a recurrent neural network to read in tokens and to generate an output sequence, as shown in Figure 1. Let us consider that the input tokens are denoted xt, and after receiving all of the input tokens a special token is used. The output tokens are denoted yt, and at training time the output tokens are fed into the network to learn proper generation of the next token. In the following equations, ht is the hidden state of a recurrent neural network, Whx is the weight matrix that computes how the input xt affects the hidden state, Whh is the weight matrix related to recurrence (i.e., how the previous hidden state affects the current hidden state), and Wyh is the weight matrix used to predict which token should be output given the hidden state. All weights are learned with supervised learning and back-propagation:

åœ¨ç¥ç»æœºå™¨ç¿»è¯‘ä¸­ï¼Œå ä¸»å¯¼åœ°ä½çš„æŠ€æœ¯è¢«ç§°ä¸ºâ€œé¡ºåº-é¡ºåºå­¦ä¹ â€ï¼Œå…¶ä¸­â€œé¡ºåºâ€æŒ‡çš„æ˜¯å¥å­ä¸­å•è¯çš„é¡ºåºã€‚åºåˆ—åˆ°åºåˆ—ç½‘ç»œ[5]çš„æ—©æœŸç¤ºä¾‹ä½¿ç”¨é€’å½’ç¥ç»ç½‘ç»œæ¥è¯»å–ä»¤ç‰Œå¹¶ç”Ÿæˆè¾“å‡ºåºåˆ—ï¼Œå¦‚å›¾1æ‰€ç¤ºã€‚è®©æˆ‘ä»¬å‡è®¾è¾“å…¥ä»¤ç‰Œè¡¨ç¤ºä¸ºxtï¼Œåœ¨æ¥æ”¶åˆ°æ‰€æœ‰è¾“å…¥ä»¤ç‰Œä¹‹åï¼Œä½¿ç”¨ä¸€ä¸ªç‰¹æ®Šçš„ä»¤ç‰Œã€‚è¾“å‡ºä»¤ç‰Œè®°ä½œytï¼Œè®­ç»ƒæ—¶å°†è¾“å‡ºä»¤ç‰Œè¾“å…¥ç½‘ç»œï¼Œå­¦ä¹ ä¸‹ä¸€ä¸ªä»¤ç‰Œçš„æ­£ç¡®ç”Ÿæˆã€‚å¼ä¸­ï¼Œhtä¸ºé€’å½’ç¥ç»ç½‘ç»œçš„éšçŠ¶æ€ï¼ŒWhxä¸ºè®¡ç®—è¾“å…¥xtå¦‚ä½•å½±å“éšçŠ¶æ€çš„æƒçŸ©é˜µï¼ŒWhhä¸ºä¸é€’å½’(å³ï¼Œå‰ä¸€ä¸ªéšè—çŠ¶æ€å¦‚ä½•å½±å“å½“å‰çš„éšè—çŠ¶æ€)ï¼ŒWyhæ˜¯ç”¨æ¥é¢„æµ‹ç»™å®šéšè—çŠ¶æ€ä¸‹åº”è¯¥è¾“å‡ºå“ªä¸ªä»¤ç‰Œçš„æƒå€¼çŸ©é˜µã€‚æ‰€æœ‰æƒå€¼é€šè¿‡ç›‘ç£å­¦ä¹ å’Œåå‘ä¼ æ’­è¿›è¡Œå­¦ä¹ :

A softmax function is then used to turn the yt values into probabilities to choose the most likely token from a learned vocabulary. In this example, one can see how the weight matrices capture the learning of common patterns; after processing the input sequence, the hidden state h encodes the most likely initial token to begin the output and each subsequent ht uses the W matrices to predict the most likely next token given the input as well as preceding tokens just produced in the output. The W matrices thus learn the long range dependencies in the full input.

ç„¶åä½¿ç”¨ä¸€ä¸ªsoftmaxå‡½æ•°å°†ytå€¼è½¬æ¢ä¸ºæ¦‚ç‡ï¼Œä»å·²å­¦ä¹ çš„è¯æ±‡è¡¨ä¸­é€‰æ‹©æœ€å¯èƒ½çš„ä»¤ç‰Œã€‚åœ¨è¿™ä¸ªä¾‹å­ä¸­ï¼Œå¯ä»¥çœ‹åˆ°æƒé‡çŸ©é˜µæ˜¯å¦‚ä½•æ•è·å¸¸è§æ¨¡å¼çš„å­¦ä¹ çš„;åœ¨å¤„ç†è¾“å…¥åºåˆ—ä¹‹åï¼Œéšè—çŠ¶æ€hç¼–ç æœ€å¯èƒ½çš„åˆå§‹ä»¤ç‰Œå¼€å§‹è¾“å‡ºï¼Œéšåçš„æ¯ä¸ªhtä½¿ç”¨WçŸ©é˜µæ¥é¢„æµ‹æœ€å¯èƒ½çš„ä¸‹ä¸€ä¸ªä»¤ç‰Œä»¥åŠåˆšåˆšåœ¨è¾“å‡ºä¸­äº§ç”Ÿçš„å‰ä¸€ä¸ªä»¤ç‰Œã€‚è¿™æ ·ï¼ŒWçŸ©é˜µå°±å¯ä»¥å­¦ä¹ æ•´ä¸ªè¾“å…¥ä¸­çš„é•¿æœŸä¾èµ–å…³ç³»ã€‚

A problem with the sequence generation described above is that only tokens which are in the training set are available for output as yt. In the case of natural human language, words such as proper names (e.g., Chicago, Stockholm) may be so rare that they do not appear in the training vocabulary, but those words may be necessary for proper output. One successful approach to overcome the vocabulary problem is to use a copy mechanism [9]. The basic intuition behind this approach is that rare words not available in the vocabulary (i.e., unknown words, referred as ), may be directly copied from the input sentence over to the output translated sentence. This relatively simple idea can be successful in many cases - especially when translating sentences containing proper names - where these tokens can be easily copied over.

ä¸Šé¢æè¿°çš„åºåˆ—ç”Ÿæˆçš„ä¸€ä¸ªé—®é¢˜æ˜¯ï¼Œåªæœ‰è®­ç»ƒé›†ä¸­çš„ä»¤ç‰Œå¯ä»¥ä½œä¸ºytè¾“å‡ºã€‚åœ¨äººç±»çš„è‡ªç„¶è¯­è¨€ä¸­ï¼Œåƒä¸“æœ‰åè¯(å¦‚èŠåŠ å“¥ã€æ–¯å¾·å“¥å°”æ‘©)è¿™æ ·çš„è¯å¯èƒ½éå¸¸ç½•è§ï¼Œä»¥è‡³äºå®ƒä»¬ä¸ä¼šå‡ºç°åœ¨è®­ç»ƒè¯æ±‡è¡¨ä¸­ï¼Œä½†è¿™äº›è¯å¯èƒ½æ˜¯æ­£ç¡®è¾“å‡ºæ‰€å¿…éœ€çš„ã€‚è§£å†³è¯æ±‡è¡¨é—®é¢˜çš„ä¸€ä¸ªæˆåŠŸæ–¹æ³•æ˜¯ä½¿ç”¨ä¸€ä¸ªå¤åˆ¶æœºåˆ¶[9]ã€‚è¿™ç§æ–¹æ³•èƒŒåçš„åŸºæœ¬ç›´è§‰æ˜¯ï¼Œè¯æ±‡è¡¨ä¸­æ²¡æœ‰ç½•è§çš„å•è¯(ä¾‹å¦‚:ï¼ŒæŠŠä¸è®¤è¯†çš„è¯ï¼Œç®€ç§°ä¸º)ï¼Œå¯ä»¥ç›´æ¥ä»è¾“å…¥çš„å¥å­ä¸­æŠ„åˆ°è¾“å‡ºçš„ç¿»è¯‘å¥å­ä¸­å»ã€‚è¿™ä¸ªç›¸å¯¹ç®€å•çš„æƒ³æ³•åœ¨å¾ˆå¤šæƒ…å†µä¸‹éƒ½æ˜¯æˆåŠŸçš„â€”â€”å°¤å…¶æ˜¯åœ¨ç¿»è¯‘å«æœ‰ä¸“æœ‰åè¯çš„å¥å­æ—¶â€”â€”è¿™äº›æ ‡è®°å¾ˆå®¹æ˜“è¢«å¤åˆ¶ã€‚
(æ—­ä¼¦æ³¨ï¼šçœ‹ä¸æ‡‚çš„ç…§æŠ„)

For example, letâ€™s consider the task of translating the following English sentence "The car is in Chicago" to French. Letâ€™s also assume that all the tokens in the sentence are in the vocabulary, except "Chicago". An NMT model might output the following sentence: "La voiture est Ã  ". With a copy mechanism, the model would be able to automatically replace the unknown token with one of the tokens from the input sentence, in this case, "Chicago".

ä¾‹å¦‚ï¼Œè®©æˆ‘ä»¬è€ƒè™‘å°†ä¸‹é¢çš„è‹±è¯­å¥å­â€œthe car is in Chicagoâ€ç¿»è¯‘æˆæ³•è¯­çš„ä»»åŠ¡ã€‚æˆ‘ä»¬è¿˜å‡è®¾å¥å­ä¸­çš„æ‰€æœ‰æ ‡è®°éƒ½åœ¨è¯æ±‡è¡¨ä¸­ï¼Œé™¤äº†â€œChicagoâ€ã€‚ä¸€ä¸ªNMTæ¨¡å‹å¯èƒ½è¾“å‡ºä»¥ä¸‹å¥å­:â€œLa voiture est aâ€ã€‚ä½¿ç”¨å¤åˆ¶æœºåˆ¶ï¼Œæ¨¡å‹å°†èƒ½å¤Ÿç”¨è¾“å…¥è¯­å¥ä¸­çš„ä¸€ä¸ªä»¤ç‰Œè‡ªåŠ¨æ›¿æ¢æœªçŸ¥çš„ä»¤ç‰Œï¼Œåœ¨æœ¬ä¾‹ä¸­æ˜¯â€œChicagoâ€ã€‚

The copy mechanism can be particularly relevant for source code, where the size of the vocabulary can be several times the size of a natural language corpus [13]. This results from the fact that developers are not constrained by any vocabulary (e.g., English dictionary) when defining names for variables or methods. This leads to an extremely large vocabulary containing many rare tokens, used infrequently only in specific contexts. Thus, the copy mechanism applied to source code allows a system to generate rare out-ofvocabulary identifier names and numeric values as long as they are somewhere in the input. Furthermore, in natural language, a human recipient may be able to use context to cope with one missing word in an automatically translated sentence. In a programming language, the compiler does not make any semantic inference, and the generation has to be complete. For example, if the code to predict is "if (i < num_cars)", then generating "if (i < int)" is not going to work at all. We discuss the mathematics of the copy mechanism in the context of SEQUENCER in Section 3.3.1. Readers interested in more detail are referred to the work by See et al. [9].

å¤åˆ¶æœºåˆ¶å¯èƒ½ä¸æºä»£ç ç‰¹åˆ«ç›¸å…³ï¼Œå…¶ä¸­è¯æ±‡è¡¨çš„å¤§å°å¯èƒ½æ˜¯è‡ªç„¶è¯­è¨€åº“[13]çš„å‡ å€ã€‚è¿™æ˜¯å› ä¸ºåœ¨ä¸ºå˜é‡æˆ–æ–¹æ³•å®šä¹‰åç§°æ—¶ï¼Œå¼€å‘äººå‘˜ä¸å—ä»»ä½•è¯æ±‡è¡¨(ä¾‹å¦‚ï¼Œè‹±è¯­è¯å…¸)çš„çº¦æŸã€‚è¿™å¯¼è‡´äº†ä¸€ä¸ªåŒ…å«è®¸å¤šç¨€æœ‰æ ‡è®°çš„éå¸¸å¤§çš„è¯æ±‡è¡¨ï¼Œè¿™äº›æ ‡è®°åªåœ¨ç‰¹å®šçš„ä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨ã€‚å› æ­¤ï¼Œåº”ç”¨äºæºä»£ç çš„å¤åˆ¶æœºåˆ¶å…è®¸ç³»ç»Ÿç”Ÿæˆç½•è§çš„è¯æ±‡è¡¨å¤–æ ‡è¯†ç¬¦åç§°å’Œæ•°å€¼ï¼Œåªè¦å®ƒä»¬ä½äºè¾“å…¥ä¸­çš„æŸä¸ªä½ç½®ã€‚æ­¤å¤–ï¼Œåœ¨è‡ªç„¶è¯­è¨€ä¸­ï¼Œäººç±»æ¥å—è€…å¯èƒ½èƒ½å¤Ÿä½¿ç”¨ä¸Šä¸‹æ–‡æ¥å¤„ç†ä¸€ä¸ªè‡ªåŠ¨ç¿»è¯‘çš„å¥å­ä¸­ç¼ºå°‘çš„å•è¯ã€‚åœ¨ç¼–ç¨‹è¯­è¨€ä¸­ï¼Œç¼–è¯‘å™¨ä¸è¿›è¡Œä»»ä½•è¯­ä¹‰æ¨ç†ï¼Œç”Ÿæˆå¿…é¡»æ˜¯å®Œæ•´çš„ã€‚ä¾‹å¦‚ï¼Œå¦‚æœè¦é¢„æµ‹çš„ä»£ç æ˜¯â€œif (i < num_cars)â€ï¼Œé‚£ä¹ˆç”Ÿæˆâ€œif (i < int)â€å°±æ ¹æœ¬ä¸èµ·ä½œç”¨ã€‚æˆ‘ä»¬åœ¨ç¬¬3.3.1èŠ‚ä¸­è®¨è®ºäº†éŸ³åºå™¨ä¸Šä¸‹æ–‡ä¸­çš„å¤åˆ¶æœºåˆ¶çš„æ•°å­¦ã€‚å¯¹æ›´å¤šç»†èŠ‚æ„Ÿå…´è¶£çš„è¯»è€…å¯ä»¥å‚è€ƒSeeç­‰äººçš„è‘—ä½œã€‚

Tufano et al. [10] proposed using NMT with the goal of learning bug-fixing patches by translating the entire buggy method into the corresponding fixed method. Before the translation, the authors perform a code abstraction process which transforms the source code into an abstracted version, which contains: (i) Java keywords and identifiers; (ii) frequent identifiers and literals (a selection of 300 idioms); (iii) typified IDs (e.g., METHOD_1, VAR_2) that replace identifiers and literals in the code. In Section 6 we highlight differences and improvements introduced in SEQUENCER.

å¤åˆ¶æœºåˆ¶å¯èƒ½ä¸æºä»£ç ç‰¹åˆ«ç›¸å…³ï¼Œå…¶ä¸­è¯æ±‡è¡¨çš„å¤§å°å¯èƒ½æ˜¯è‡ªç„¶è¯­è¨€åº“[13]çš„å‡ å€ã€‚è¿™æ˜¯å› ä¸ºåœ¨ä¸ºå˜é‡æˆ–æ–¹æ³•å®šä¹‰åç§°æ—¶ï¼Œå¼€å‘äººå‘˜ä¸å—ä»»ä½•è¯æ±‡è¡¨(ä¾‹å¦‚ï¼Œè‹±è¯­è¯å…¸)çš„çº¦æŸã€‚è¿™å¯¼è‡´äº†ä¸€ä¸ªåŒ…å«è®¸å¤šç¨€æœ‰æ ‡è®°çš„éå¸¸å¤§çš„è¯æ±‡è¡¨ï¼Œè¿™äº›æ ‡è®°åªåœ¨ç‰¹å®šçš„ä¸Šä¸‹æ–‡ä¸­ä½¿ç”¨ã€‚å› æ­¤ï¼Œåº”ç”¨äºæºä»£ç çš„å¤åˆ¶æœºåˆ¶å…è®¸ç³»ç»Ÿç”Ÿæˆç½•è§çš„è¯æ±‡è¡¨å¤–æ ‡è¯†ç¬¦åç§°å’Œæ•°å€¼ï¼Œåªè¦å®ƒä»¬ä½äºè¾“å…¥ä¸­çš„æŸä¸ªä½ç½®ã€‚æ­¤å¤–ï¼Œåœ¨è‡ªç„¶è¯­è¨€ä¸­ï¼Œäººç±»æ¥å—è€…å¯èƒ½èƒ½å¤Ÿä½¿ç”¨ä¸Šä¸‹æ–‡æ¥å¤„ç†ä¸€ä¸ªè‡ªåŠ¨ç¿»è¯‘çš„å¥å­ä¸­ç¼ºå°‘çš„å•è¯ã€‚åœ¨ç¼–ç¨‹è¯­è¨€ä¸­ï¼Œç¼–è¯‘å™¨ä¸è¿›è¡Œä»»ä½•è¯­ä¹‰æ¨ç†ï¼Œç”Ÿæˆå¿…é¡»æ˜¯å®Œæ•´çš„ã€‚ä¾‹å¦‚ï¼Œå¦‚æœè¦é¢„æµ‹çš„ä»£ç æ˜¯â€œif (i < num_cars)â€ï¼Œé‚£ä¹ˆç”Ÿæˆâ€œif (i < int)â€å°±æ ¹æœ¬ä¸èµ·ä½œç”¨ã€‚æˆ‘ä»¬åœ¨ç¬¬3.3.1èŠ‚ä¸­è®¨è®ºäº†éŸ³åºå™¨ä¸Šä¸‹æ–‡ä¸­çš„å¤åˆ¶æœºåˆ¶çš„æ•°å­¦ã€‚å¯¹æ›´å¤šç»†èŠ‚æ„Ÿå…´è¶£çš„è¯»è€…å¯ä»¥å‚è€ƒSeeç­‰äººçš„è‘—ä½œã€‚

Tufano et al. [10] proposed using NMT with the goal of learning bug-fixing patches by translating the entire buggy method into the corresponding fixed method. Before the translation, the authors perform a code abstraction process which transforms the source code into an abstracted version, which contains: (i) Java keywords and identifiers; (ii) frequent identifiers and literals (a selection of 300 idioms); (iii) typified IDs (e.g., METHOD_1, VAR_2) that replace identifiers and literals in the code. In Section 6 we highlight differences and improvements introduced in SEQUENCER.

Tufanoç­‰äºº[10]æå‡ºä½¿ç”¨NMTï¼Œç›®çš„æ˜¯é€šè¿‡å°†æ•´ä¸ªbugæ–¹æ³•è½¬æ¢ä¸ºç›¸åº”çš„å›ºå®šæ–¹æ³•æ¥å­¦ä¹ bugä¿®å¤è¡¥ä¸ã€‚åœ¨è½¬æ¢ä¹‹å‰ï¼Œä½œè€…æ‰§è¡Œä¸€ä¸ªä»£ç æŠ½è±¡è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹å°†æºä»£ç è½¬æ¢ä¸ºä¸€ä¸ªæŠ½è±¡ç‰ˆæœ¬ï¼Œè¯¥ç‰ˆæœ¬åŒ…å«:(i) Javaå…³é”®å­—å’Œæ ‡è¯†ç¬¦;(ii)ç»å¸¸ä½¿ç”¨çš„æ ‡è¯†ç¬¦å’Œæ–‡å­—(300ä¸ªä¹ è¯­);(iii)æ›¿æ¢ä»£ç ä¸­çš„æ ‡è¯†ç¬¦å’Œæ–‡å­—çš„ç±»å‹åŒ–id(å¦‚METHOD_1ã€VAR_2)ã€‚åœ¨ç¬¬6èŠ‚ä¸­ï¼Œæˆ‘ä»¬é‡ç‚¹ä»‹ç»äº†éŸ³åºå™¨ä¸­çš„å·®å¼‚å’Œæ”¹è¿›ã€‚

Another approach to addressing the vocabulary size problem in code is to use byte pair encoding (BPE), which has been widely used in NLP and also applied to source code [14]. For SEQUENCER, we did preliminary experiments with BPE to solve the unlimited vocabulary problem, but our early results showed that it is less effective than the copy mechanism.

è§£å†³ä»£ç ä¸­è¯æ±‡è¡¨å¤§å°é—®é¢˜çš„å¦ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨å­—èŠ‚å¯¹ç¼–ç  (BPE)ï¼Œå®ƒå·²åœ¨ NLP ä¸­å¹¿æ³›ä½¿ç”¨ï¼Œä¹Ÿåº”ç”¨äºæºä»£ç  [14]ã€‚å¯¹äºéŸ³åºå™¨ï¼Œæˆ‘ä»¬ç”¨ BPE åšäº†åˆæ­¥çš„å®éªŒæ¥è§£å†³æ— é™è¯æ±‡é‡çš„é—®é¢˜ï¼Œä½†æ˜¯æˆ‘ä»¬æ—©æœŸçš„ç»“æœè¡¨æ˜å®ƒæ¯”å¤åˆ¶æœºåˆ¶çš„æ•ˆæœå·®ã€‚

## 3 APPROACH TO USING SEQ-TO-SEQ LEARNING FOR REPAIR

SEQUENCER is a sequence-to-sequence deep learning model that aims at automatically fixing bugs by generating oneline patches (i.e., the bug can be fixed by replacing a single buggy line with a single fixed line). We do not consider line deletion because: 1) it does not require a method for token generation (and is thus less interesting to our research) and 2) if desired, SEQUENCER could be combined with the lightweight Kali [11] to include line deletion. We do not consider line addition because spectrum based fault localization, used in most of the related work, is not effective for line addition patches [15]. We note that in 64% of all 395 bugs in Defects4J are fixed by replacing existing source code [16]. Given a Software System with a faulty behavior (i.e., failing test case), state-of-the-art fault localization techniques are used to identify the buggy method and the suspicious buggy lines. Such techniques have been shown to predict the correct buggy line as one of the top 10 candidates in 44% of the time [15]. SEQUENCER then performs a novel Buggy Context Abstraction (Section 3.2) process which intelligently organizes the fault localization data (i.e., buggy classes, methods, and lines) into a representation that is concise and suitable for the deep learning model yet able to preserve valuable information regarding the context of the bug, which will be used to predict the fix. The representation is then fed to a trained sequence-to-sequence model (Section 3.3.1) which performs Patch Inference (Section 3.4) and is capable of generating multiple single-lines of code that represent the potential one-line patches for the bug. Finally, SEQUENCER in the Patch Preparation (Section 3.5) step generates the concrete patches by formatting the code and replacing the suspicious line with the proposed lines. Figure 2 shows the aforementioned steps both for the training phase (left) and inference phase (right). In the remainder of this section we will discuss the common steps as well as those specific for training and inference.

åºåˆ—å‘ç”Ÿå™¨æ˜¯ä¸€ä¸ªåºåˆ—åˆ°åºåˆ—çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œæ—¨åœ¨é€šè¿‡ç”Ÿæˆä¸€è¡Œè¡¥ä¸æ¥è‡ªåŠ¨ä¿®å¤é”™è¯¯ (iã€‚e.,è¿™ä¸ªé”™è¯¯å¯ä»¥é€šè¿‡ç”¨ä¸€æ¡å›ºå®šçš„çº¿æ›¿æ¢ä¸€æ¡é”™è¯¯çº¿æ¥ä¿®å¤)ã€‚æˆ‘ä»¬ä¸è€ƒè™‘è¡Œåˆ é™¤ï¼Œå› ä¸º: 1) å®ƒä¸éœ€è¦ä»¤ç‰Œç”Ÿæˆçš„æ–¹æ³• (å› æ­¤å¯¹æˆ‘ä»¬çš„ç ”ç©¶æ¥è¯´ä¸é‚£ä¹ˆæœ‰è¶£)ï¼Œ2) å¦‚æœéœ€è¦,éŸ³åºå™¨å¯ä»¥ä¸è½»é‡çº§ Kali [11] ç›¸ç»“åˆï¼ŒåŒ…æ‹¬çº¿è·¯åˆ é™¤ã€‚æˆ‘ä»¬ä¸è€ƒè™‘çº¿è·¯æ·»åŠ ï¼Œå› ä¸ºåœ¨å¤§å¤šæ•°ç›¸å…³å·¥ä½œä¸­ä½¿ç”¨çš„åŸºäºé¢‘è°±çš„æ•…éšœå®šä½å¯¹äºçº¿è·¯æ·»åŠ è¡¥ä¸ [15] æ— æ•ˆã€‚æˆ‘ä»¬æ³¨æ„åˆ°ï¼Œåœ¨ Defects4J çš„æ‰€æœ‰ 64% ä¸ªé”™è¯¯ä¸­ï¼Œæœ‰ 395 æ˜¯é€šè¿‡æ›¿æ¢ç°æœ‰çš„æºä»£ç  [16] æ¥ä¿®å¤çš„ã€‚ç»™å®šå…·æœ‰é”™è¯¯è¡Œä¸ºçš„è½¯ä»¶ç³»ç»Ÿ (i.e.,å¤±è´¥çš„æµ‹è¯•æ¡ˆä¾‹)ï¼Œæœ€å…ˆè¿›çš„æ•…éšœå®šä½æŠ€æœ¯è¢«ç”¨äºè¯†åˆ«è¶Šé‡è½¦æ–¹æ³•å’Œå¯ç–‘çš„è¶Šé‡è½¦çº¿è·¯ã€‚è¿™ç§æŠ€æœ¯å·²ç»è¢«è¯æ˜å¯ä»¥é¢„æµ‹æ­£ç¡®çš„è¶Šé‡è½¦çº¿ï¼Œæˆä¸º 44% [15] çš„å‰ 10 åå€™é€‰äººä¹‹ä¸€ã€‚æ’åºå™¨ç„¶åæ‰§è¡Œä¸€ä¸ªæ–°é¢–çš„è¶Šé‡è½¦ä¸Šä¸‹æ–‡æŠ½è±¡ (ç¬¬ 3.2 èŠ‚) è¿‡ç¨‹ï¼Œè¯¥è¿‡ç¨‹æ™ºèƒ½åœ°ç»„ç»‡æ•…éšœå®šä½æ•°æ® (i e.,é”™è¯¯çš„ç±»ã€æ–¹æ³•å’Œè¡Œ) æˆä¸ºä¸€ä¸ªè¡¨ç¤ºï¼Œè¯¥è¡¨ç¤ºç®€æ´ä¸”é€‚åˆæ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œä½†èƒ½å¤Ÿä¿å­˜æœ‰å…³ bug ä¸Šä¸‹æ–‡çš„æœ‰ä»·å€¼çš„ä¿¡æ¯,å°†ç”¨äºé¢„æµ‹ä¿®å¤ã€‚ç„¶åï¼Œè¯¥è¡¨ç¤ºè¢«é¦ˆé€åˆ°ç»è¿‡è®­ç»ƒçš„åºåˆ—åˆ°åºåˆ—æ¨¡å‹ (ç¬¬ 3.3.1 èŠ‚)ï¼Œè¯¥æ¨¡å‹æ‰§è¡Œè¡¥ä¸æ¨ç† (ç¬¬ 3.4 èŠ‚) å¹¶ä¸”èƒ½å¤Ÿç”Ÿæˆå¤šä¸ªä»£è¡¨ bug æ½œåœ¨å•è¡Œè¡¥ä¸çš„å•è¡Œä»£ç ã€‚æœ€åï¼Œè¡¥ä¸å‡†å¤‡ (ç¬¬ 3.5 èŠ‚) æ­¥éª¤ä¸­çš„å®šåºå™¨é€šè¿‡æ ¼å¼åŒ–ä»£ç å¹¶ç”¨å»ºè®®çš„è¡Œæ›¿æ¢å¯ç–‘è¡Œæ¥ç”Ÿæˆå…·ä½“çš„è¡¥ä¸ã€‚å›¾ 2 æ˜¾ç¤ºäº†è®­ç»ƒé˜¶æ®µ (å·¦) å’Œæ¨ç†é˜¶æ®µ (å³) çš„ä¸Šè¿°æ­¥éª¤ã€‚åœ¨æœ¬èŠ‚çš„å‰©ä½™éƒ¨åˆ†ï¼Œæˆ‘ä»¬å°†è®¨è®ºå¸¸è§çš„æ­¥éª¤ä»¥åŠç‰¹å®šäºè®­ç»ƒå’Œæ¨ç†çš„æ­¥éª¤ã€‚

[11] Z. Qi, F. Long, S. Achour, and M. Rinard, â€œAn analysis of patch plausibility and correctness for generate-andvalidate patch generation systems,â€ in Proceedings of the 2015 International Symposium on Software Testing and Analysis, ser. ISSTA 2015, Baltimore, MD, USA: ACM, 2015, pp. 24â€“36, ISBN: 978-1-4503-3620-8. DOI: 10.1145/ 2771783.2771791. [Online]. Available: http://doi.acm.org/10.1145/2771783.2771791.

[15] D. Zou, J. Liang, Y. Xiong, M. D. Ernst, and L. Zhang, â€œAn empirical study of fault localization families and their combinations,â€ IEEE Transactions on Software Engineering, 2019.


