Hardware Approximate Techniques for Deep Neural Network Accelerators: A Survey

- GIORGOS ARMENIAKOS, National Technical University of Athens, Greece
- GEORGIOS ZERVAKIS, Karlsruhe Institute of Technology, Germany
- DIMITRIOS SOUDRIS, National Technical University of Athens, Greece
- JÖRG HENKEL, Karlsruhe Institute of Technology, Germany

Deep Neural Networks (DNNs) are very popular because of their high performance in various cognitive tasks in Machine Learning (ML). Recent advancements in DNNs have brought levels beyond human accuracy in many tasks, but at the cost of high computational complexity. To enable efficient execution of DNN inference, more and more research works, therefore, are exploiting the inherent error resilience of DNNs and employing Approximate Computing (AC) principles to address the elevated energy demands of DNN accelerators. This article provides a comprehensive survey and analysis of hardware approximation techniques for DNN accelerators. First, we analyze the state of the art, and by identifying approximation families, we cluster the respective works with respect to the approximation type. Next, we analyze the complexity of the performed evaluations (with respect to the dataset and DNN size) to assess the efficiency, potential, and limitations of approximate DNN accelerators. Moreover, a broad discussion is provided regarding error metrics that are more suitable for designing approximate units for DNN accelerators as well as accuracy recovery approaches that are tailored to DNN inference. Finally, we present how Approximate Computing for DNN accelerators can go beyond energy efficiency and address reliability and security issues as well.

深度神经网络（DNN）因其在机器学习（ML）的各种认知任务中具有高性能而非常受欢迎。 DNN的最新进展在许多任务中带来了超越人类准确度的水平，但代价是高计算复杂度。为了实现DNN推理的高效执行，越来越多的研究工作因此利用DNN的固有误差容忍性，并采用近似计算（AC）原理来解决DNN加速器的升高的能量需求。本文全面调查和分析了用于DNN加速器的硬件逼近技术。首先，我们分析了最新技术水平，并通过识别逼近家族，将相应的工作与逼近类型相对应地进行聚类。接下来，我们分析了执行评估的复杂性（与数据集和DNN大小相关），以评估近似DNN加速器的效率，潜力和限制。此外，还提供了广泛的讨论，涉及更适合设计DNN加速器的近似单元的误差度量标准，以及适用于DNN推理的准确度恢复方法。最后，我们展示了近似计算如何超越能源效率，解决可靠性和安全性问题。

## 1 INTRODUCTION

Advancements in Deep Learning (DL) with Deep Neural Networks (DNNs) have delivered beyond human levels of accuracy on many AI tasks [125]. An increasing number of embedded devices rely on DL and DNNs to deliver sophisticated services such as machine translation [15], object detection [73], healthcare [9, 77], and so forth. However, these accuracy improvements came at the cost of a vast increase in computational demands, leading to the emergence of customized hardware DNN accelerators [58, 125]. It is noteworthy that recent Convolution Neural Networks (CNNs) require tens of billions of multiply-accumulate (MAC) operations [125]. To satisfy such demands, DNN accelerators integrate thousands of MAC units; e.g., Google TPU [58] comprises 64KMACs, while Samsung’s neural processing unit (NPU) contains 6KMAC units [91]. This immense number of MAC units combined with high parallelization results in high energy demands. This problem is intensified especially when considering the growth of Edge AI that requires even more complex neural networks (NNs) to operate on a wide spectrum of energy- and resourcerestricted devices.

深度学习（DL）和深度神经网络（DNN）的进展已经在许多人工智能任务上实现了超越人类水平的准确性[125]。越来越多的嵌入式设备依赖于DL和DNN来提供复杂的服务，例如机器翻译[15]、物体检测[73]、医疗保健[9, 77]等。然而，这些准确性的提高是以巨大的计算需求为代价的，导致定制硬件DNN加速器的出现[58, 125]。值得注意的是，最近的卷积神经网络（CNN）需要数十亿次乘加（MAC）操作[125]。为满足这样的需求，DNN加速器集成了数千个MAC单元；例如，Google TPU [58] 包含64KMAC，而三星的神经处理单元（NPU）包含6KMAC单元[91]。这么多的MAC单元结合高度并行化会导致高能耗的问题。特别是考虑到边缘人工智能的增长，需要在广泛的能源和资源受限设备上运行更复杂的神经网络（NNs），这个问题变得更加严重。

Over the past decade, Approximate Computing (AC) [42] has been established as a new design paradigm for energy-efficient circuits. AC goes beyond typical/emerging design approaches [96] and exploits the inherent ability of a large number of applications to produce results of acceptable quality, despite some errors (approximations) in their computations. Leveraging this property, AC approximates the hardware execution of the error-resilient computations in a manner that favors performance and energy [125]. Driven by the high potential for energy efficiency and exploiting the error tolerance of NNs [124, 136], research on approximate NN implementations has been rapidly growing over the last years. Figure 1 is a representative example of this trend. Figure 1 depicts the number of publications in three major design automation conferences that apply approximations in CNN inference.

在过去的十年中，近似计算（AC）[42]已被确立为一种新的设计范式，用于能效电路。AC超越了典型/新兴设计方法[96]，利用了大量应用程序产生可接受质量结果的固有能力，尽管计算中存在一些错误（近似）。利用这个特性，AC以有利于性能和能源的方式逼近具有容错能力的计算的硬件执行[125]。由于近似NN实现具有高能效潜力并利用NN的误差容忍性[124, 136]，近年来近似NN实现的研究迅速增长。图1是这一趋势的一个代表性例子。图1描绘了应用CNN推理中的逼近方法的三个主要设计自动化会议的发表文章数量。

Considering the high demand for edge AI [45], the billions ofmobile devices running DNN inference, and the rapid growth of AI chips,1 our focus in this survey is to study, analyze, and elucidate the impact of hardware approximation techniques on the efficiency and accuracy of DNN inference accelerators. Prior research on DNN accelerators reports that between 30% and 80% of the system energy is consumed by DRAM [63], with data movement dominating the energy consumption [130]. Still, the processing units (e.g., MACs) of DNN accelerators feature considerable power consumption [4, 125]. Hence, considering high utilization and continuous operation, high energy is also consumed by the processing units and could be prohibitive, for example, in battery-powerembedded devices [125]. In addition, the very high power consumed by the processing units in a confined area may lead to unsustainable power densities with far-reaching impact on the temperature, performance, and reliability of DNN accelerators [4]. Although several works examine approximate memories for DNNs [28, 63, 64, 107], such works are out of the scope of our survey, which focuses on computational approximation. Note, nevertheless, that compute-based (our survey) and memory-based approximations are mainly complementary. Finally, although approximate computing mainly targets energy efficiency in DNN accelerators (Sections 3 to 5), several works apply approximations to tackle reliability and security issues (Section 6).

考虑到对边缘人工智能[45]的高需求，运行DNN推理的数十亿移动设备和人工智能芯片的快速增长，我们在本调查中的重点是研究、分析和阐明硬件逼近技术对DNN推理加速器的效率和准确性的影响。先前关于DNN加速器的研究报告指出，30%到80%的系统能量被DRAM消耗[63]，数据移动占据了能量消耗的主导地位[130]。然而，DNN加速器的处理单元（例如MAC）具有相当大的功耗[4, 125]。因此，考虑到高利用率和持续运行，处理单元也会消耗大量能量，这可能会对例如电池供电的嵌入式设备[125]造成限制。此外，处理单元在有限的区域内消耗的非常高的功率可能导致不可持续的功率密度，对DNN加速器的温度、性能和可靠性产生深远影响[4]。虽然有几篇论文研究了DNN的近似存储器[28, 63, 64, 107]，但这些工作不在我们调查的范围内，我们的调查重点是计算逼近。然而，需要注意的是，基于计算的（我们的调查）和基于存储器的逼近主要是相互补充的。最后，尽管近似计算主要针对DNN加速器的能效性（第3至5节），但有几篇论文应用逼近方法来解决可靠性和安全性问题（第6节）。

The ever-increasing demand for efficient DNN inference and the prominent outcomes of AC applications have attracted significant research interest. As shown in Table 1, several surveys address similar topics to our work. A survey of approximate arithmetic units (e.g., adders and multipliers) is presented in [56]. Nevertheless, in [56], only a simple DNN use case example is used as a proof of concept. On the other hand, [98] presents a comprehensive study of approximate circuits, discussing also DNN-specific approximation techniques. However, in [98], softwarebased approximation techniques (such as quantization and pruning) are mainly reviewed, while regarding hardware-based approximation, only a limited discussion based on approximate multipliers is included. In [125] and [14] the impact of DNN approximation techniques is reviewed with the main focus on software-based approaches. In [17], a survey of DNN accelerator architectures is provided, while [12] reviews hardware and software optimization methods for DNN accelerators. Similarly to [12], [116] and [27] present very comprehensive surveys on software optimizations/approximations and hardware architectures for DNNs. However, hardware DNN approximations are not the target of [12, 17, 27, 116]. Finally, [32, 71] present a thorough analysis of software-based approximation methods such as quantization and pruning, while [99] provides a comprehensive review of recent NN architectures. Approximate DNN accelerators are out of the scope of [32, 71, 99]. On the other hand, our work surveys the state of the art of approximate DNN accelerators. Specifically, our work focuses and provides in-depth discussion of DNN-specific approximate techniques that are implemented at the hardware level (e.g., logic approximation) and/or modify the architecture of the accelerator.

对于高效的DNN推理的需求不断增长，AC应用的显著成果吸引了相当大的研究兴趣。正如表1所示，有几篇综述涉及类似的主题。[56]提供了近似算术单元（例如加法器和乘法器）的综述。然而，在[56]中，仅使用了一个简单的DNN用例示例作为概念证明。另一方面，[98]提供了对近似电路的全面研究，还讨论了DNN特定的近似技术。然而，在[98]中，主要回顾了基于软件的逼近技术（如量化和剪枝），而在硬件逼近方面，仅包括基于近似乘法器的有限讨论。在[125]和[14]中，回顾了DNN逼近技术的影响，主要关注于基于软件的方法。在[17]中，提供了DNN加速器体系结构的调查，而[12]则回顾了DNN加速器的硬件和软件优化方法。与[12]类似，[116]和[27]对DNN的软件优化/逼近和硬件体系结构进行了非常全面的调查。然而，[12、17、27、116]并不针对硬件DNN逼近。最后，[32、71]对诸如量化和剪枝等基于软件的逼近方法进行了深入分析，而[99]则提供了对最近NN体系结构的全面审查。[32、71、99]不涉及近似DNN加速器。另一方面，我们的研究调查了近似DNN加速器的最新技术现状。具体而言，我们的研究重点关注并深入讨论了实现在硬件级别上的DNN特定的逼近技术（例如逻辑逼近）和/或修改加速器的体系结构。

## 2 BRIEF BACKGROUND ON DEEP NEURAL NETWORKS

Deep neural networks consist of artificial neurons. The computation model of a neuron is illustrated in Figure 2 and given by Equation (1). Each neuron performs a weighted sum of all its inputs and then a bias term is added for a possible offset [12]. The result is passed through the activation function, from which the output of the neuron is obtained. Neurons are represented as nodes in a graph and are organized in layers. In DL, a layer is a function that receives inputs from the previous layers and passes outputs to the next layers [33]. It is usually uniform, and it only consists of one type of activation function, pooling, convolution, and so forth.

深度神经网络由人工神经元组成。神经元的计算模型如图2所示，并由方程（1）给出。每个神经元对其所有输入执行加权和，然后添加一个偏置项以进行可能的偏移[12]。结果通过激活函数进行处理，从中获取神经元的输出。神经元表示为图中的节点，并以层的形式组织。在深度学习中，一个层是一个接收来自前一层输入并将输出传递给下一层的函数[33]。它通常是均匀的，仅包含一种类型的激活函数、池化、卷积等。

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/neural.png)

$y_j=\Phi\left(\sum_{k=0}^{n-1} x_k w_{k j}+b\right)$

where $y_j$ is the output of the neuron, $w_{kj}$ are the neuron’s weights, n is the number of weights, $x_k$ are the neuron’s inputs, b is the bias of the neuron, and Φ is the activation function.

The most popular and widely used neural networks today are Multi-Layer Perceptrons, Convolutional Neural Networks, Recurrent Neural Networks, and Transformers [58, 125]. Specifically:

(1) Multi-Layer Perceptrons (MLPs): Each node in a layer is composed of a nonlinear function of a weighted sum of all the previous outputs (fully connected) [116].

(2) Convolutional Neural Networks: They are mainly composed of convolutional, pooling, and fully connected layers and exploit the concept of shared weights and are designed to learn spatial hierarchies of features [116].

(3) Recurrent Neural Networks (RNNs): Each layer is composed of nonlinear functions of the weighted sums of the outputs and the previous state. Long Short-Term Memory (LSTM) is the most common RNN. The weights are reused across time steps. A key feature of LSTMs is to decide what to forget and what to forward to the next layer [50].

(4) Transformers: They handle sequential input data as RNNs, but they differ since they use a different mechanism called “self-attention” that weights the significance of each input part and enables parallel data processing [113].

目前最流行和广泛使用的神经网络包括多层感知器、卷积神经网络、循环神经网络和Transformer [58, 125]。具体而言：

(1) 多层感知器（MLPs）：每个层中的节点是先前所有输出的加权和的非线性函数（完全连接）[116]。

(2) 卷积神经网络：主要由卷积、池化和全连接层组成，利用共享权重的概念，旨在学习空间特征的层次结构[116]。

(3) 循环神经网络（RNNs）：每个层是输出和先前状态的加权和的非线性函数。长短时记忆（LSTM）是最常见的RNN。权重在时间步长中被重复使用。LSTM的一个关键特征是决定忘记什么并将什么转发到下一层[50]。

(4) Transformer：它们处理序列输入数据，就像RNNs一样，但它们使用称为“自注意力”的不同机制，加权每个输入部分的重要性，并实现并行数据处理 [113]。

The goal of our work is to survey the state of the art of hardware approximation techniques for DNN accelerators, without any constraints on the DNN type, though, as will be shown in Section 5.2, the majority of the examined works mainly use only CNNs in their analysis/evaluation.

我们的工作的目标是调查用于DNN加速器的硬件逼近技术的最新技术，没有对DNN类型的任何限制，尽管如第5.2节所示，大部分研究仅在其分析/评估中使用CNNs。

### 2.1 Layers

### 2.2 Training and Inference

### 2.3 Models and Datasets

## 3 HARDWARE APPROXIMATIONS FOR DNNS

In this section, the state of the art of hardware approximate computing techniques mainly for deep CNN inference is discussed. Note that although some of these techniques rely on (re)training to mitigate the accuracy loss due to approximation, training is used only as a mechanism to improve the accuracy of the approximate inference and it is not the target of the approximation itself. In addition, after identifying common patterns in examined techniques, we organize them in groups with respect to the type of applied approximation. As illustrated in Figure 5, hardware DNN approximation can be clustered into three wide categories: Computation Reduction, Approximate (Arithmetic) Units, and Precision Scaling. It is noteworthy that although these approximation categories are orthogonal, the state of the art applies, mainly, approximations from one category or combines Precision Scaling with approximations from another category.

在本节中，主要讨论了用于深度CNN推理的硬件近似计算技术的最新技术。请注意，尽管其中一些技术依赖于（重新）训练以减轻由逼近引起的精度损失，但训练仅用作提高近似推理的精度的机制，而不是逼近本身的目标。此外，在确定了检查的技术中的共同模式后，我们根据应用的逼近类型将它们组织成组。如图5所示，硬件DNN逼近可以分为三个广泛的类别：计算减少、近似（算术）单元和精度缩放。值得注意的是，尽管这些逼近类别是正交的，但现有技术主要应用来自一个类别的逼近或将精度缩放与另一个类别的逼近相结合。

### 3.1 Precision Scaling

Low-precision computation is the key to enable high compute densities in DNN hardware accelerators across cloud and edge platforms. One of the first and most widely used approximation techniques to enable effective precision scaling is quantization. Quantized hardware implementations feature reduced bitwidth dataflow and arithmetic units (as illustrated in Figure 6), thus attaining very high energy, latency, and bandwidth gains compared to 32-bit floating-point (FP32) implementations. Traditionally FP32 was used in DNN inference. Rather than executing all the required mathematical operations with ordinary 32-bit/16-bit floating point (as in CPUs and GPUs), quantization allows us to exploit smaller integer operations instead. Moreover, quantized implementations reduce the size of the model linearly, leading to high storage gains and low memory transfers. In integer-arithmetic-only inference, weights and activations are quantized to lowbitwidth (e.g., 8-bit) integers and biases are quantized to 32-bit or lower [55]. Other quantization approaches mainly target model compression and quantize only the weights, e.g., [137]. Advancements in quantization methods have demonstrated that integer 8-bit (INT8) DNN inference can achieve almost identical accuracy with FP32 [58]. Finally, a significant advantage of quantization is that although it directly impacts the hardware requirements, the accuracy loss is fully controlled and defined at the software level. In other words, the hardware gains will depend only on the supported precision(s) of the accelerator, while the accuracy will depend on the employed quantization method, though the latter assumes that the accumulators of the DNN accelerator have enough precision to avoid any overflow and accurately accumulate the partial sums [18, 40, 58, 116]. If this is not the case, then approximate results may be obtained since the intermediate partial summight be clipped by a maximum value defined by the precision of the accumulator. However, the works that we studied in this survey do not consider such an approximation and the size of the accumulator is selected large enough to avoid any overflow, e.g., based on the largest filter size. Concluding, studying quantizationmethods and quantized hardware implementations is out of the scope of thiswork, and comprehensive discussions can be found in many works [14, 20, 21, 27, 32, 71, 98, 116, 125]. A brief discussion is included in this section for completeness reasons and since many of the approximate techniques discussed hereafter are compatible and/or orthogonal with quantized implementations. Nevertheless, quantized implementations will not be further analyzed.

低精度计算是在云端和边缘平台上实现DNN硬件加速器高计算密度的关键。其中一种最早和最广泛使用的近似技术是量化。量化硬件实现采用了降低位宽的数据流和算术单元（如图6所示），与32位浮点（FP32）实现相比，能够获得非常高的能量、延迟和带宽收益。传统上，在DNN推断中使用FP32。量化使我们能够使用较小的整数操作来执行所有所需的数学运算，而不是使用普通的32位/16位浮点数（如在CPU和GPU中）。此外，量化实现线性地减小了模型的大小，从而获得高存储收益和低内存传输。在仅使用整数算术的推断中，权重和激活值被量化为低位宽（例如8位）整数，而偏置被量化为32位或更低的整数[55]。其他量化方法主要针对模型压缩，仅对权重进行量化，例如[137]。量化方法的进展已经证明，整数8位（INT8）的DNN推断可以实现几乎与FP32相同的准确性[58]。最后，量化的一个重要优势是，尽管它直接影响硬件要求，但准确性损失完全在软件层面进行控制和定义。换句话说，硬件收益仅取决于加速器支持的精度，而准确性则取决于所采用的量化方法，尽管后者假设DNN加速器的累加器具有足够的精度以避免溢出并准确地累加偏差和部分和[18, 40, 58, 116]。如果不是这种情况，则可能获得近似结果，因为中间的部分和可能会被累加器精度定义的最大值截断。然而，我们在这项调查中研究的工作并未考虑此类近似，并且累加器的大小足够大，以避免任何溢出，例如基于最大滤波器大小进行选择。总之，研究量化方法和量化硬件实现超出了本工作的范围，有关此方面的综合讨论可以在许多文献中找到[14, 20, 21, 27, 32, 71, 98, 116, 125]。为了完整起见，本节包含了一个简要讨论，并且由于后续讨论的许多近似技术与量化实现是兼容和/或正交的。然而，将不再进一步分析量化实现。

Post-training Quantization:

Quantization-aware Training:

Binary/Ternary Quantization:

### 3.2 Computation Reduction

During DNN inference, millions of multiplications are performed in the convolution operations [125], leading to high latency and energy consumption, even when considering quantized implementations. The Computation Reduction approximation category aims at systematically avoiding, at the hardware level, the execution of some computations, e.g., multiplications and convolution operations. As a result, it significantly decreases the executed workload. Computation Reduction is further subdivided into the Memoization and Skipping approximation families. Computation Reduction (illustrated in Figure 7) uses a conditional statement to avoid a computation and estimate its output (Memoization) or discard it entirely (Skipping). Among the most popular, effective, and extensively used DNN approximation techniques that reduce the number of the required computations is the software-based DNN pruning. Pruning removes connections, filters, and/or channels based on varying importance criteria and can be divided into structured (coarse-grained) and fine-grained pruning. Pruning actually generates a compressed variant of the initial network and is executed offline before inference. On the other hand, in the hardware approximation techniques that we study in this section, the approximation originates from the hardware itself since a conditional statement is integrated in the accelerator and decides at runtime if a computation will be skipped/estimated or not. Hence, although several architectures exist, e.g., with zero-skipping support, to optimally support the software-based pruning approximation [43, 90], such architectures are not inherently approximate since they will skip computations that do not need to be executed (e.g., multiplication by zero), while the examined Computation Reduction approximation techniques will skip computations that many times should be executed in order to obtain full accuracy.

在DNN推断过程中，卷积操作中会执行数百万次乘法[125]，即使考虑量化实现，也会导致高延迟和能量消耗。计算量缩减逼近类别旨在在硬件级别上系统地避免执行一些计算，例如乘法和卷积操作。因此，它显著减少了执行的工作量。计算量缩减进一步分为记忆化和跳过逼近族群。计算量缩减（如图7所示）使用条件语句避免计算并估计其输出（记忆化）或完全丢弃它（跳过）。在减少所需计算数量的最流行、有效和广泛使用的DNN逼近技术中，最常用的是基于软件的DNN剪枝。剪枝基于不同的重要性标准删除连接、滤波器和/或通道，可以分为结构化（粗粒度）和细粒度剪枝。剪枝实际上生成初始网络的压缩变体，并在推断之前离线执行。另一方面，在本节研究的硬件逼近技术中，逼近来自硬件本身，因为条件语句集成在加速器中，并在运行时决定是否跳过/估计计算。因此，尽管存在几种体系结构，例如具有零跳过支持的体系结构，以最佳方式支持基于软件的剪枝逼近[43，90]，但这样的体系结构并不是本质上逼近的，因为它们将跳过不需要执行的计算（例如乘以零），而被考虑的计算量缩减逼近技术将跳过许多次应该执行的计算以获得全面的准确性。

#### 3.2.1 Skipping.

Skipping approximations aim at reducing the executed workload. Such approaches perform a simple computation and evaluate (predict) if a more complex one can be eliminated. Hence, this approximation family enables dynamic approximation at runtime. The efficiency of the Skipping approximation relies on howoften a computation can be skipped, the complexity of the conditional prediction, and the complexity of the skipped operation. Piyasena et al. [94] leverage the widely used ReLu activation function to eliminate redundant computations. [94] estimates the sign of the convolution output using a low-cost prediction scheme. In this scheme, a powerof- two weight quantization is applied so that multiplications can be replaced with simple logic shifters. If the estimated sign of the approximate output is negative, the convolution operation is skipped through the clock-gated circuitry, or else the original convolution is performed. [119] proposes a similar strategy, but the sign estimation is done either after representing weights in ternary format or after using a sign function, which simplifies the computations while maintaining the prediction accuracy. Kim and Seo [61] exploit the max-pooling layers and adopt a precision-cascading scheme to predict and calculate only the maximum value of a convolution operation. This technique, combined with a zero-skipping scheme, can efficiently avoid redundant computations without affecting neuron synapses that contribute a lot in classification accuracy. In [48], the weights of each layer of a given CNN are clustered offline in groups. K-means is used for clustering and weights within a cluster feature the highest similarity to each other while weights of different clusters exhibit the least similarity. During inference (i.e., at runtime), only someweight groups are used while the weights of the rest groups are assumed to be zero. Finally, the difference of the two output neurons with the highest values is calculated. If the difference is above a given threshold, the obtained prediction is the output of [48], or else the inference is repeated using gradually more weight groups. Finally, Huan et al. [52] introduced Near-Zero Approximation (NZA). NZA exploits the fact that when the multiplication operands are very small (close to zero), the product will be almost zero. [52] counts the leading zeros of the multiplication operands and if their number is above a threshold, the product is assumed to be zero and the multiplication is skipped.

跳过逼近技术旨在减少执行的工作量。这些方法执行简单的计算并评估（预测）是否可以消除更复杂的计算。因此，这种逼近族群使动态逼近在运行时成为可能。跳过逼近的效率取决于计算可以被跳过的频率、条件预测的复杂度以及被跳过操作的复杂度。Piyasena等人[94]利用广泛使用的ReLu激活函数消除冗余计算。[94]使用低成本预测方案估计卷积输出的符号。在此方案中，应用二的幂权重量化，以便可以用简单的逻辑移位器替换乘法。如果近似输出的估计符号为负，则通过时钟门电路跳过卷积操作，否则执行原始卷积操作。[119]提出了类似的策略，但符号估计是在将权重表示为三进制格式或使用符号函数后进行的，这样可以简化计算同时保持预测精度。Kim和Seo[61]利用最大池化层，并采用精度级联方案来预测和计算卷积操作的最大值。这种技术与零跳过方案相结合，可以有效避免冗余计算，而不影响在分类准确性方面发挥重要作用的神经元突触。在[48]中，给定CNN的每个层的权重被离线聚类成组。使用K均值进行聚类，组内的权重彼此之间具有最高的相似性，而不同组的权重具有最少的相似性。在推断期间（即在运行时），仅使用一些权重组，而其余组的权重被假定为零。最后，计算两个具有最高值的输出神经元之间的差异。如果差异高于给定阈值，则获得的预测是[48]的输出，否则使用逐渐增加的重量组重复推断。最后，Huan等人[52]引入了近零逼近（NZA）。NZA利用了当乘法操作数非常小（接近于零）时，乘积将几乎为零的事实。[52]计算乘法操作数的前导零数，如果它们的数量超过阈值，则假定乘积为零并跳过乘法运算。

#### 3.2.2 Memoization.

The second subcategory of the Computation Reduction is Memoization. Memoization avoids a computation (e.g., multiplication or convolution) by replacing its output with the output of a previously performed similar computation. Hence, the efficiency of this approach depends on the input similarity (i.e., how often a replacement takes place) as well as the complexity of the eliminated computation. Jiao et al. [57] apply Memoization through a configurable BloomFilter (BF) unit that stores the product of frequently computed patterns and avoids performing the respective multiplications. A memoization set of 3,000 images was used in [57] to identify such patterns. Mocerino et al. [76] proposed a CAM-enhanced floating-point unit (FPU) to implement Memoization. Pre-computed multiplication results are reused whenever a similar input pattern occurs, thus avoiding unnecessary computations of frequent operations. To increase the frequency of patterns, a clustering approach based on the Jenks Natural Breaks algorithm is applied to weights and activations. The processing unit of [76] is pipelined and consists of two CAMs (one for the weights and one for the activations) and an SRAM. If the input pattern is precomputed, the product is loaded from the memory and the multiplier is avoided by clock-gated signals. On the other hand, [101] showed that more than 60% of the inputs of the network layer exhibit negligible changes with respect to the previous execution. Based on that fact, they proposed a method to reuse some results from the previous execution, avoiding all the computations associated with those results.

计算量缩减的第二个子类是记忆化。记忆化通过将其输出替换为先前执行的类似计算的输出来避免计算（例如乘法或卷积）。因此，这种方法的效率取决于输入的相似性（即替换发生的频率）以及被消除的计算的复杂度。Jiao等人[57]通过可配置的BloomFilter（BF）单元应用记忆化，该单元存储经常计算的模式的乘积，并避免执行相应的乘法。在[57]中使用了一个记忆化集合，其中包括3,000张图像，用于识别这些模式。Mocerino等人[76]提出了一种增强型CAM浮点单元（FPU）来实现记忆化。当出现类似的输入模式时，会重用预先计算的乘法结果，从而避免频繁操作的不必要计算。为了增加模式的频率，基于Jenks Natural Breaks算法的聚类方法被应用于权重和激活。[76]的处理单元被分阶段处理，并由两个CAM（一个用于权重，一个用于激活）和一个SRAM组成。如果输入模式是预先计算的，则从存储器中加载乘积并通过时钟门控信号避免乘法器。另一方面，[101]显示，网络层的60%以上的输入相对于先前的执行具有可忽略的变化。基于这个事实，他们提出了一种方法来重用先前执行的某些结果，从而避免与这些结果相关的所有计算。

### 3.3 Approximate Units

DNN hardware accelerators comprise thousands of MAC units [58]. This wide category improves the energy consumption and/or latency of DNN accelerators by employing approximate circuits that replace accurate MAC units (Figure 8). Approximate Units can be further divided into three approximation families: Approximate Multipliers/Adders, Multiplierless, and Approximate Log- Multipliers. Briefly, Approximate Multipliers/Adders modify the circuit implementation of the multiplier/ adder (e.g., logic approximation), Multiplierless replaces the multiplication with a simpler operation (e.g., addition), and the Approximate Log-Multipliers family replaces the exact binary multiplier with a logarithmic multiplier that is further approximated.

DNN硬件加速器由数千个MAC单元组成[58]。这个广泛的类别通过使用近似电路来替代精确的MAC单元（图8）来改善DNN加速器的能耗和/或延迟。近似单元可以进一步分为三个近似家族：近似乘法器/加法器、无乘法器和近似对数乘法器。简而言之，近似乘法器/加法器修改了乘法器/加法器的电路实现（例如逻辑近似），无乘法器用简单的操作（例如加法）替换了乘法运算，而近似对数乘法器家族则将精确的二进制乘法器替换为进一步近似的对数乘法器。

![](https://xulun-mooc.oss-cn-beijing.aliyuncs.com/appro.png)

#### 3.3.1 Approximate Multipliers/Adders.

Considering the vast number of MAC operations required in the inference phase, several works focus on approximating the circuit of the MAC unit itself. Exploiting a constant energy gain per MAC operation performed, very high energy gains are obtained at the inference level. Targeting approximate MAC circuits, the state of the art mainly approximates the multiplier, since it is more complex and power consuming than the adder [10, 78, 108, 134].

考虑到推断阶段需要大量的MAC运算，有几项研究专注于对MAC单元本身的电路进行近似。通过利用每个MAC操作的恒定能量增益，在推断级别上可以获得非常高的能量收益。在针对近似MAC电路的研究中，现有技术主要对乘法器进行近似，因为乘法器比加法器更复杂且耗电量更大[10, 78, 108, 134]。

Mrazek et al. [79] employ a Cartesian genetic programming (CGP)-based optimization— since it is intrinsically multi-objective and produces efficient approximate arithmetic circuits [78]—to generate approximate multipliers for inference accelerators [79]. The multipliers generated by [79] satisfy a given worst-case error constraint and ensure that multiplication by 0 is always accurate. An iterative optimization procedure is used to identify the error constraint for the generation of approximate multipliers in CGP optimization so that an inference accuracy loss threshold is satisfied. During the iterative procedure, after replacing the accurate multipliers with the approximate ones, the network is retrained to obtain the best-quality results [79]. Similarly, Vasicek et al. [121] use CGP-based optimization to generate approximate multipliers. In order to avoid time consuming CNN evaluation during the optimization phase, [121] used the Weighted Mean Error Distance (WMED) metric to quantify the accuracy of the approximate multipliers. To calculateWMED, the significance of each error is determined by the probability mass function of the network’s weight distribution. Ansari et al. [6] evaluated 600 approximate multipliers (500 CGP-based ones and 100 variants of deliberately designed multipliers) in CNN inference showing that they can deliver significant gains in terms of power and area for a minimal accuracy loss. Moreover, [6] discussed that the induced approximation noise helps to mitigate the overfitting problem, and thus can even improve the obtained accuracy. After analyzing 600 approximate multipliers, a significant conclusion of [6] showed that when designing approximate multipliers for CNN inference, the most important error metrics are the error variance and the root mean square error. Similar to [79], [121] and [6] apply retraining to mitigate the accuracy loss due to the approximate multiplications. Nevertheless, approximation-aware retraining can be very time consuming, as discussed in Section 2.2.

Mrazek等人[79]采用基于Cartesian遗传编程（CGP）的优化方法，因为它在本质上是多目标的，并且能够生成高效的近似算术电路[78]，用于生成推断加速器的近似乘法器[79]。[79]生成的乘法器满足给定的最坏情况误差约束，并确保对0的乘法始终准确无误。采用迭代优化过程来确定CGP优化中的近似乘法器生成的误差约束，以满足推断准确性损失的阈值。在迭代过程中，将准确的乘法器替换为近似乘法器后，对网络进行重新训练以获得最佳质量结果[79]。类似地，Vasicek等人[121]使用基于CGP的优化方法生成近似乘法器。为了避免在优化阶段耗时的CNN评估，[121]使用加权均方误差距离（WMED）度量来量化近似乘法器的准确性。计算WMED时，每个误差的重要性由网络权重分布的概率质量函数确定。Ansari等人[6]在CNN推断中评估了600个近似乘法器（其中包括500个基于CGP的乘法器和100个故意设计的乘法器变体），表明它们可以在最小准确性损失的情况下带来显著的功耗和面积收益。此外，[6]还指出，引入的近似噪声有助于缓解过拟合问题，甚至可以改善获得的准确性。在分析了600个近似乘法器之后，[6]得出了一个重要结论，即在设计用于CNN推断的近似乘法器时，最重要的误差度量指标是误差方差和均方根误差。类似于[79]、[121]和[6]，重新训练被应用来减轻由于近似乘法引起的准确性损失。然而，对于近似感知的重新训练可能非常耗时，正如第2.2节中讨论的那样。

Mrazek et al. [80] extended the EvoApprox8b library [78] and generated 8 × N-bit approximate multipliers. CGP-optimization and the quality metric of [79] are used for the generation of the approximate multipliers. [80] evaluated the generated approximate multipliers in CNN inference. Through a comprehensive analysis, [80] demonstrated that for less complex CNNs (ResNets [46] on CIFAR10), approximate multipliers may deliver considerable power savings for minimal accuracy loss (even without retraining). A similar approach that aims to eliminate multiplications by quantizing one term in power-of-two format is presented in [74]. In this method, during the forward pass weights are converted in ternary format, while in backpropagation weights and activations are quantized up to 4 bits to improve the accuracy. Nevertheless, this is not the case for more complex CNNs (ResNet-164 on CIFAR100), where even for 10% energy reduction the accuracy loss is considerable. Leveraging that weights are known after training, CAxCNN [100] uses the Canonic Sign Digit (CSD) representation to encode the weights. CSD uses ternary form {−1, 0, 1}, and to represent a binary number, CSD features the least number of non-zeros {−1, 1}. In addition, adjacent bits cannot be both non-zero. Exploiting these two features of CSD, [100] applied truncation and generated approximate CSD multipliers with a very small footprint as well as low latency. Although [100] performs an optimization search to identify the optimal truncation parameter, CAxCNN does not require retraining. Exploiting that different layers feature varying resilience to approximation, ALWANN [81] applied a non-uniform approximation. ALWANN generates a heterogeneous DaDianNao architecture [16] by using heterogeneous processing elements (PEs). The employed PEs are built upon different approximate multipliers from EvoApproxLib [78]. ALWANN [81] implements a layer-wise approximation in which each layer is mapped to a specific PE type.A genetic optimization procedure is used to identify the approximate multiplier per PE aswell as the layer mapping to PEs. ALWANN avoids retraining and recovers some of the accuracy loss by employing a simple, approximation-awareweight-tuning procedure. Similarly, Zervakis et al. [131] also applied layer-wise approximation. [131] used wire-by-switch replacement to generate an approximate multiplier with three accuracy (relative error) modes. Hence, using this reconfigurable multiplier, [131] generated a homogeneous approximate architecture. Through an exhaustive exploration, [131] determined the accuracy mode per convolution layer and generated the respective accuracy-energy consumption Pareto front. Tasoulas et al. [118] introduced the weight-oriented approximation. [118] generated a low-variance approximatemultiplier (LVRM) with three approximation modes (i.e., three error variance values). A greedy procedure is used in [118] to map weight ranges to the approximation modes of LVRM. The significance of each convolution layer is also used in the mapping procedure; i.e., weights of less sensitive layers are entirely mapped to the highest approximation. In addition, [118] proposed a bias correction method in order to avoid retraining and mitigate the accuracy loss due to the approximate multiplications. Hammad et al. [41] performed approximate multiplication using the Dynamic and Static Segmented Multipliers (DSM, SSM), which perform the multiplication with m-bit input segments (where m is smaller than the input bitwidth). In SSM the most significant segment that contains a 1 is used (static), while in DSM the segment is dynamically selected based on a leading one detector (LOD). To attain high accuracy, [41] generated a reconfigurable accelerator that comprises low-precision (low m) and high-precision (highm) approximate multipliers. A low-cost classifier is trained to predict the required precision (low or high) for each input image. At runtime, a controller decides the precision level and then inference is executed using the respective approximate multipliers. Guo et al. [38] proposed an approximate multiplier that can support one 16-by-8-bit multiplication or two 16-by-4-bit multiplications and uses an approximate adder to add/merge the outputs of the sub-multiplications. The proposed approximate adder extends the block-based adder GeAr [108]. [38] observed that in a quantized CNN, the inputs of the multipliers roughly follow a Gaussian distribution instead of a uniform distribution. Exploiting the correlation of the bits for Gaussian distributed inputs, [38] generated approximate adders with an unequally sized block structure to trade off between accuracy and circuit delays. [38] considers an Eyeriss-like architecture [18] that uses the proposed approximate reconfigurable multipliers and employs different quantization precision for different layers (i.e., 8-bit or 4-bit). Exploiting the proposed reconfigurable approximate multiplication, layers with 4-bit weights are executed at higher throughput.

Mrazek等人[80]扩展了EvoApprox8b库[78]，生成了8×N位的近似乘法器。使用CGP优化和[79]的质量度量来生成近似乘法器。[80]在CNN推断中评估了生成的近似乘法器。通过全面分析，[80]证明对于较简单的CNN（在CIFAR10上的ResNet[46]），近似乘法器可能在最小准确性损失的情况下实现相当大的功耗节约（甚至无需重新训练）。[74]中介绍了一种类似的方法，旨在通过将幂次为二的格式量化一个项来消除乘法。在这种方法中，在前向传播过程中，权重被转换为三进制格式，而在反向传播过程中，权重和激活被量化为4位，以提高准确性。然而，对于更复杂的CNN（在CIFAR100上的ResNet-164），即使为了降低10%的能量消耗，准确性损失也是相当大的。利用训练后已知权重的特点，CAxCNN [100]使用Canonic Sign Digit（CSD）表示来编码权重。CSD使用三进制形式{-1，0，1}，为了表示二进制数，CSD具有最少数量的非零数{-1，1}。此外，相邻位不能同时为非零。利用CSD的这两个特点，[100]应用截断并生成了具有非常小的占用空间和低延迟的近似CSD乘法器。虽然[100]执行了优化搜索以确定最佳的截断参数，但CAxCNN不需要重新训练。利用不同层对近似的鲁棒性有所不同的特点，ALWANN [81]应用了非均匀近似。ALWANN通过使用异构处理单元（PE）构建了异构的DaDianNao架构[16]。所采用的PE是基于EvoApproxLib [78]中的不同近似乘法器构建的。ALWANN [81]实施了一种逐层逼近的方法，其中每一层都被映射到特定的PE类型。遗传优化过程用于确定每个PE的近似乘法器以及层映射到PE的过程。ALWANN通过简单的、近似感知的权重调整过程避免了重新训练，并恢复了部分由于近似乘法引起的准确性损失。类似地，Zervakis等人[131]也应用了逐层逼近。[131]使用线路开关替换来生成具有三种准确性（相对误差）模式的近似乘法器。因此，利用这个可重配置的乘法器，[131]生成了一个均匀的近似架构。通过详尽的探索，[131]确定了每个卷积层的准确性模式，并生成了相应的准确性-能耗 Pareto 前沿。Tasoulas等人[118]引入了面向权重的近似。[118]生成了具有三种近似模式（即三个误差方差值）的低方差近似乘法器（LVRM）。在[118]中使用贪婪算法将权重范围映射到LVRM的近似模式。映射过程中还使用了每个卷积层的重要性；即，对于不太敏感的层的权重完全映射到最高的近似。此外，[118]提出了一种偏差校正方法，以避免重新训练并减轻近似乘法引起的准确性损失。Hammad等人[41]使用动态和静态分段乘法器（DSM，SSM）进行近似乘法，它们使用m位输入段（其中m小于输入位宽）进行乘法计算。在SSM中，使用包含1的最高位段（静态），而在DSM中，根据前导1检测器（LOD）动态选择段。为了获得高准确性，[41]生成了一个可重配置的加速器，其中包括低精度（低m）和高精度（高m）的近似乘法器。训练了一个低成本分类器，用于预测每个输入图像所需的精度（低或高）。在运行时，控制器决定精度级别，然后使用相应的近似乘法器执行推断。Guo等人[38]提出了一种近似乘法器，可以支持一个16×8位乘法或两个16×4位乘法，并使用近似加法器来添加/合并子乘法的输出。所提出的近似加法器扩展了基于块的加法器GeAr [108]。[38]观察到在量化的CNN中，乘法器的输入大致上遵循高斯分布而不是均匀分布。利用高斯分布输入的位之间的相关性，[38]生成了具有不均匀大小的块结构的近似加法器，以在准确性和电路延迟之间进行权衡。[38]考虑了类似于Eyeriss的架构[18]，使用所提出的近似可重配置乘法器，并对不同层使用不同的量化精度（即8位或4位）。利用所提出的可重配置近似乘法，具有4位权重的层以更高的吞吐量执行。

Hanif et al. [44] consider a systolic MAC array architecture [58] and introduce a curable approximation technique. “Curable” approximation refers to approximation approaches that feature an internal error compensation mechanism that enables them to self-correct the induced error. This is mainly achieved by estimating the error at runtime and compensating it at a later stage. CANN [44] splits the adder of the MAC unit into two parts (low and high) and cancels the carry propagation from the low to the high part. Hence, the carry chain (and thus the delay) of the MAC unit is decreased. To cure the introduced error, the output carry of the low part is accumulated in the next cycle by the neighboring MAC unit. The errors generated by the eliminated carries of the border MAC units are not cured. Zervakis et al. [133] considered also a systolic MAC array architecture [58] and replaced the accurate multipliers with the approximate perforated ones [134]. The perforated multipliers omit the generation of some partial products and thus the induced error is known a priori [134]. [133] introduced a control variate approximation technique to heal the approximate multiplication error at runtime. [133] leverages that the weights are known after training and that the error of the perforated multipliers can be rigorously expressed in order to formulate a control variate that efficiently estimates the runtime convolution error based on the values of the input activations. An additional column of MAC units is required to accumulate the control variate and compensate the error.

Hanif等人[44]考虑了一种并行MAC阵列架构[58]并引入了一种可纠正的近似技术。"可纠正"的近似指的是具有内部误差补偿机制的近似方法，使其能够自行纠正引入的误差。主要通过在运行时估计误差并在后续阶段进行补偿来实现。CANN [44]将MAC单元的加法器分为两部分（低位和高位），取消了从低位到高位的进位传播。因此，MAC单元的进位链（以及延迟）减少了。为了纠正引入的误差，低位的输出进位在下一个周期由相邻的MAC单元累加。边界MAC单元消除的进位所产生的误差不被纠正。Zervakis等人[133]也考虑了一种并行MAC阵列架构[58]，并用近似的穿孔乘法器[134]替换了准确的乘法器。穿孔乘法器省略了一些部分积的生成，因此引入的误差是预先知道的[134]。[133]引入了一种控制变量的近似技术，以在运行时纠正近似乘法的误差。[133]利用训练后权重已知以及穿孔乘法器的误差可以严格表示的特点，构建了一个控制变量，根据输入激活值的数值高效估计运行时卷积误差。还需要额外的MAC单元列来累加控制变量并补偿误差。

Concluding, the integration of Approximate Multipliers/Adders in neural network accelerators has attracted significant research interest over the last years. The approximation techniques that belong in this family can be further organized as follows: 

- [6, 79, 81, 100, 121] generate NN-specific approximate accelerators, i.e., apply NN-specific approximations. On the other hand, [38, 41, 44, 80, 118, 131, 133] generate generic approximate accelerators. 

- [6, 79, 121] apply retraining to mitigate the accuracy loss, while [38, 41, 80, 81, 100, 118, 131] do not require/apply retraining, and [44, 133] employ a runtime curable approximation technique. 

- [6, 44, 79, 80, 100, 121, 133] apply static approximation, while [38, 41, 81, 118, 131] employ dynamically reconfigurable approximate architectures.

总结起来，近年来，将近似乘法器/加法器集成到神经网络加速器中引起了重大的研究兴趣。属于这个家族的近似技术可以进一步组织如下：

[6, 79, 81, 100, 121] 生成针对神经网络的特定近似加速器，即应用神经网络特定的近似方法。另一方面，[38, 41, 44, 80, 118, 131, 133] 生成通用的近似加速器。

[6, 79, 121] 应用重新训练来减轻准确性损失，而[38, 41, 80, 81, 100, 118, 131] 不需要/应用重新训练，[44, 133] 则采用了一种运行时可纠正的近似技术。

[6, 44, 79, 80, 100, 121, 133] 应用静态近似，而[38, 41, 81, 118, 131] 采用动态可重构的近似架构。

#### 3.3.2 Multiplierless.

The Multiplierless subcategory aims at maximizing the gains by eliminating the expensive multiplication circuits. To achieve this, multipliers are replaced by circuits that implement a simpler operation. Parmar and Sridharan [92] exploited the fact that scaling the input feature map does not affect the features extracted by max-pooling and reduced the span of the scaled weights to [−1, 1]. This condition allowed to introduce in the convolution equation trigonometric functions, which can be implemented by the low-cost CORDIC algorithm. In [29], authors proposed reconfigurable constant coefficient multipliers (RCCMs) that use only adders and shifters. The supported coefficients are extracted offline based on a distribution matching technique that allows specific RCCMs to be selected depending on the model’s weights. Sarwar et al. [105] employed multiplierless neurons by replacing multipliers with simplified shifts and add operations controlled by a unit. The so-called Alphabet Set Multipliers (ASMs) compose a precomputer bank to compute lower-order multiples of the input based on some small-bit sequences termed alphabets ({1, 2, 3, 5, . . .}), an adder, and one or more select, shift, and control logic units. The size of the alphabet defines the accuracy as well as the energy benefits of ASM. An efficient retraining is finally performed in order to tune the weights and mitigate the accuracy degradation due to ASMs.

Multiplierless子类旨在通过消除昂贵的乘法电路来最大化收益。为了实现这一点，乘法器被实现简化操作的电路所取代。Parmar和Sridharan [92]利用了输入特征映射的缩放不会影响最大池化提取的特征的事实，并将缩放权重的范围减小到[-1, 1]。这个条件允许在卷积方程中引入三角函数，而这些三角函数可以通过低成本的CORDIC算法实现。在[29]中，作者提出了可重构的常数系数乘法器（RCCM），它只使用加法器和移位器。支持的系数是基于分布匹配技术进行离线提取的，这允许根据模型的权重选择特定的RCCM。Sarwar等人[105]通过用简化的移位和加法操作替换乘法器，并由一个单元控制，使用无乘法器的神经元。所谓的字母集乘法器（ASMs）由预计算器库组成，根据一些称为字母表（{1, 2, 3, 5, . . .}）的小位序列，一个加法器，以及一个或多个选择、移位和控制逻辑单元来计算输入的低阶倍数。字母表的大小定义了ASM的准确性和能量收益。最后进行有效的重新训练，以调整权重并减轻由于ASMs引起的准确性下降。

#### 3.3.3 Approximate Log-Multipliers.

The Log-Multipliers subcategory converts multiplications into additions by taking an approximate logarithm. Mitchell [75] proposed an approximate multiplier that employs the log multiplication property. [75] proposed to compute approximate binary log and antilog by a linear approximation of the log-antilog curves between each powerof- two interval. Saadat et al. [103] extended [75] to generate a minimally biased approximate multiplier. [103] observed that in Mitchell’s algorithm the error value is always negative. Through a mathematical analysis, [103] demonstrated that with the addition of a constant correction term, overall the error is reduced and the average error is pushed close to zero. In addition, [103] applied truncation to reduce the size of the main components required (i.e., adder and barrel shifters). Kim et al. [62] optimized Mitchell’s logarithmic multiplier for approximate CNN inference. [62] improved Mitchell’s implementation (LOD, shift, and decoder blocks); introduced a zero-checking block, which is mandatory to improve the performance of CNNs; and further approximated the design by applying truncation and one’s complement for negation. In [93], another approximate logarithmic multiplier with two stages of approximations was proposed. During the first stage, the two operands are split into two parts and the proposed multiplier selects either the upper part (if it contains at least one non-zero bit) or the lower part (if not) for the following computations. The second stage of approximation concerns the binary-to-logarithm conversion, where, in order to reduce more the complexity of circuitry, only a number of (leftmost) bits in the mantissa part are kept. Vogel et al. [126] introduced a quantization scheme to fractional powers-of-two (e.g., 21/4) and showed that the latter provides higher resolution at higher values and the granularity of weight distribution becomes more fine-grained. Based on the proposed quantization, [126] replaced the binary MAC units with logarithmic PEs that use an adder, a lookup table (for the required exponents), and a barrel shifter before accumulating the result.

Log-Multipliers子类通过使用近似对数将乘法转换为加法。Mitchell [75]提出了一种利用对数乘法性质的近似乘法器。[75]提出通过在线性近似计算每个幂次为二间隔之间的对数-反对数曲线，来计算近似的二进制对数和反对数。Saadat等人[103]扩展了[75]以生成一个偏差最小的近似乘法器。[103]观察到在Mitchell的算法中，误差值总是负值。通过数学分析，[103]证明通过添加一个恒定的校正项，整体上减小了误差，并且平均误差被推向接近零的位置。此外，[103]应用了截断来减小所需的主要组件的大小（例如加法器和移位器）。Kim等人[62]对Mitchell的对数乘法器进行了优化，用于近似CNN推断。[62]改进了Mitchell的实现（LOD、shift和decoder块）；引入了一个零检查块，这对于改善CNN的性能是必需的；并通过应用截断和补码取反进一步近似设计。在[93]中，提出了另一个具有两个近似阶段的近似对数乘法器。在第一阶段，将两个操作数分为两部分，所提出的乘法器选择上部分（如果至少包含一个非零位）或下部分（如果没有）进行后续计算。第二阶段的近似涉及二进制到对数的转换，在此阶段为了进一步降低电路的复杂性，只保留尾数部分的一定数量（最左边）的位数。Vogel等人[126]引入了一种分数幂次为二（例如21/4）的量化方案，并展示了后者在较高值处提供了更高的分辨率，权重分布的粒度变得更加细粒度。基于提出的量化方法，[126]用对数PE替换了二进制MAC单元，该PE使用一个加法器、一个查找表（用于所需的指数）和一个累加结果之前的移位器。

## 4 ERROR COMPENSATION TECHNIQUES

### 4.1 Accuracy Recovery with Retraining

### 4.2 Statistical Error Compensation

### 4.3 Error Metric Optimization

## 5 ENERGY-ACCURACY EVALUATION

### 5.1 Assessing the Complexity of the Evaluation Scenarios

### 5.2 Performance Analysis

## 6 NOT JUST ENERGY EFFICIENCY

### 6.1 Reliability-Aware Approximation

### 6.2 Defensive Approximations

## 7 CONCLUSIONS, CHALLENGES, AND PERSPECTIVE

In this article, hardware approximation techniques for DNNs are reviewed, characterized, classified, and evaluated. Moreover, we provide a comprehensive analysis of error metrics and error mitigation approaches specific for DNN approximations in order to provide an in-depth analysis of the studied field. Note that, in addition to the traditional exploitation of Approximate Computing for energy reduction,we present howapproximate computing can be employed inDNNs to address reliability and security concerns. Our analysis clustered the hardware DNN approximation techniques into three categories: Precision Scaling, Computation Reduction, and Approximate Units.

本文回顾了DNN的硬件逼近技术，对其进行了表征、分类和评估。此外，我们还提供了针对DNN逼近的误差度量和误差缓解方法的全面分析，以提供对所研究领域的深入分析。请注意，除了传统的利用近似计算来降低能耗外，我们还介绍了如何在DNN中使用近似计算来解决可靠性和安全性问题。我们的分析将硬件DNN逼近技术分为三类：精度缩放、计算减少和近似单元。

Precision Scaling is the most widely used method and already adopted by most commercial DNN accelerators. Advancements in quantization-aware (re-)training methods have led to minimal accuracy loss even with 4-bit or 2-bit inference. Nevertheless, note that quantization-aware training can be very time consuming and post-training quantization approaches are efficient for 8-bit inference—which is considered mainstream today—and with some limitations might enable 4-bit inference.

精度缩放是最广泛使用的方法，已被大多数商用DNN加速器采用。量化感知（重新）训练方法的进展已经导致即使使用4位或2位推理也可以实现最小的精度损失。然而，请注意，量化感知训练可能非常耗时，而后训练量化方法对于8位推理是有效的——这被认为是今天的主流——并且在某些限制下可能实现4位推理。

The Computation Reduction approximation is demonstrated to deliver very high energy reduction for minimal accuracy loss. However, this approximate category mainly examines 32-bit inference, and the energy gains dropped significantly when considering more challenging evaluations such as 8-bit inference and/or ImageNet. As a result, to obtain conclusive results regarding the Computation Reduction, a more in-depth comprehensive analysis is required with respect either to more challenging evaluation scenarios or to NNs that indeed require high-precision inference.

计算减少逼近方法被证明能够在最小化精度损失的情况下提供非常高的能量节省。然而，这个逼近类别主要是针对32位推理进行的研究，当考虑到更具挑战性的评估，例如8位推理和/或ImageNet时，能源节省效果显著降低。因此，为了得出关于计算减少的确切结果，需要进行更深入的全面分析，无论是针对更具挑战性的评估场景还是需要高精度推理的神经网络。

After Precision Scaling, the Approximate Units category has attracted the highest research interest. Typically, Approximate Units are combined with low precision (mainly 8-bit inference). Again, despite the high energy gains reported in many cases, a more comprehensive and challenging evaluation is required. Although the results seem promising, evaluations on the state-of-the-art ImageNet datasets are still limited. Still, it is noteworthy that for small DNNs the Approximate Multipliers/Adders family delivers immense energy reduction with negligible accuracy loss. The latter appears ideal for IoT devices that need to run sophisticated DNN-based services. Moreover, although some works aimed at evaluating the tradeoff between low precision (8-bit and below) and approximate units in DNN inference and showed that a combination of the two outperforms the isolated application of very low precision, this correlation is not comprehensively analyzed yet. Finally, note that an inherent limitation is this category that many techniques require retraining to recover the accuracy loss. In contrast to quantization-aware training that can run efficiently on CPUs and GPUs, retraining with approximate units requires hardware emulation that can even become infeasible in complex DNNs. To avoid retraining, curable approximation and/or statistical error compensation methods appear to be very promising solutions but are still understudied.

在精度缩放之后，近似单元逼近类别引起了最高的研究兴趣。通常，近似单元与低精度（主要是8位推理）相结合。再次说明，尽管在许多情况下报告了高能量收益，但需要进行更全面和具有挑战性的评估。尽管结果看起来很有前途，但对最先进的ImageNet数据集的评估仍然有限。不过，值得注意的是，对于小型DNN，近似乘法器/加法器家族可以在几乎不损失精度的情况下实现巨大的能量节省。后者似乎非常适合需要运行复杂DNN服务的IoT设备。此外，尽管一些研究旨在评估低精度（8位及以下）和近似单元在DNN推理中的权衡，并表明两者的组合优于非常低精度的单独应用，但这种关联尚未得到全面分析。最后，请注意，这个类别的固有限制是许多技术需要重新训练以恢复精度损失。与可以在CPU和GPU上有效运行的量化感知训练不同，使用近似单元进行重新训练需要硬件仿真，甚至在复杂的DNN中可能变得不可行。为了避免重新训练，可治愈逼近和/或统计误差补偿方法似乎是非常有前途的解决方案，但仍需要进一步研究。
