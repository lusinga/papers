# Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing

This article surveys and organizes research works in a new paradigm in natural language processing, which we dub “prompt-based learning.” Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P (y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string xˆ, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website NLPedia–Pretrain including constantly updated survey and paperlist.

这篇文章回顾和组织了自然语言处理中一种新的范式的研究工作，我们称之为“基于提示的学习”。与传统的监督学习不同，它训练一个模型来接受一个输入x并预测一个输出y作为P(y|x)，基于提示的学习是基于直接建模文本概率的语言模型。为了使用这些模型来执行预测任务，原始输入x被使用一个模板修改为一个文本字符串提示x’，其中有一些未填充的槽位，然后语言模型被用来概率地填充未填充的信息，以获得一个最终字符串x^，从中可以推导出最终输出y。这种框架有很多强大和吸引人的原因：它允许语言模型在大量的原始文本上进行预训练，并通过定义一个新的提示函数，使模型能够进行少样本甚至零样本学习，适应新场景而无需或几乎不需要标注数据。在本文中，我们介绍了这种有前途的范式的基础知识，描述了一套统一的数学符号，可以涵盖各种现有的工作，并沿着几个维度组织了现有的工作，例如预训练语言模型、提示和调优策略的选择。为了使感兴趣的初学者更容易接触这个领域，我们不仅系统地回顾了现有的工作和一个高度结构化的基于提示的概念分类，还发布了其他资源，例如一个包含持续更新的综述和论文列表的网站NLPedia–Pretrain。
