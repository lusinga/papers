# Survey of Hallucination in Natural Language Generation

Natural Language Generation (NLG) has improved exponentially in recent years thanks to the development of sequence-to-sequence deep learning technologies such as Transformer-based language models. This advancement has led to more fluent and coherent NLG, leading to improved development in downstream tasks such as abstractive summarization, dialogue generation and data-to-text generation. However, it is also apparent that deep learning based generation is prone to hallucinate unintended text, which degrades the system performance and fails to meet user expectations in many real-world scenarios. To address this issue, many studies have been presented in measuring and mitigating hallucinated texts, but these have never been reviewed in a comprehensive manner before.

自然语言生成（NLG）在近年来得益于序列到序列的深度学习技术的发展，如基于Transformer的语言模型，而呈现出指数级的提升。这种进步使得NLG更加流畅和连贯，从而促进了下游任务如摘要生成、对话生成和数据到文本生成的发展。然而，也很明显，基于深度学习的生成容易产生一些无意的文本，即幻觉（hallucination），这会降低系统的性能，并不能满足用户在许多真实场景中的期望。为了解决这个问题，许多研究提出了衡量和减轻幻觉文本的方法，但这些方法从未被全面地回顾过。

In this survey, we thus provide a broad overview of the research progress and challenges in the hallucination problem in NLG. The survey is organized into two parts: (1) a general overview of metrics, mitigation methods, and future directions; and (2) an overview of task-specific research progress on hallucinations in the following downstream tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, machine translation, and visual-language generation. This survey serves to facilitate collaborative efforts among researchers in tackling the challenge of hallucinated texts in NLG.

在这篇综述中，我们提供了一个关于NLG中幻觉问题的研究进展和挑战的广泛概述。这篇综述分为两部分：（1）关于评价指标、缓解方法和未来方向的一般概述；（2）关于以下下游任务中幻觉问题的任务特定研究进展的概述，即摘要生成、对话生成、生成式问答、数据到文本生成、机器翻译和视觉-语言生成。这篇综述旨在促进研究者之间在解决NLG中幻觉文本的挑战方面的协作努力。

## 1 INTRODUCTION

Natural Language Generation (NLG) is one of the crucial yet challenging sub-fields of Natural Language Processing (NLP). NLG techniques are used in many downstream tasks such as summarization, dialogue generation, generative question answering (GQA), data-to-text generation, and machine translation. Recently, the rapid development of NLG has captured the imagination of many thanks to the advances in deep learning technologies, especially Transformer [189]-based models like BERT [29], BART [100], GPT-2 [149], and GPT-3 [16]. The conspicuous development of NLG tasks attracted the attention of many researchers, leading to an increased effort in the field.

自然语言生成（NLG）是自然语言处理（NLP）的关键而又具有挑战性的子领域之一。NLG技术被用于许多下游任务，如摘要、对话生成、生成式问答（GQA）、数据到文本生成和机器翻译。最近，由于深度学习技术的进步，尤其是基于Transformer [189]的模型，如BERT [29]、BART [100]、GPT-2 [149]和GPT-3 [16]，NLG的快速发展引起了许多人的想象。NLG任务的显著发展引起了许多研究人员的关注，导致该领域的研究工作增加。

Alongside the advancement of NLG models, attention towards their limitations and potential risks has also increased. Some early works [71, 201] focus on the potential pitfalls of utilizing the standard likelihood maximization-based objective in training and decoding of NLG models. They discovered that such likelihood maximization approaches could result in degeneration, which refers generated output that is bland, incoherent, or gets stuck in repetitive loops. Concurrently, it is discovered that NLG models often generate text that is nonsensical, or unfaithful to the provided source input [85, 153, 159, 190]. Researchers started referring to such undesirable generation as hallucination [125].

随着NLG模型的进步，对它们的局限性和潜在风险的关注也增加了。一些早期的工作[71, 201]关注了在训练和解码NLG模型时利用标准的似然最大化目标的潜在缺陷。他们发现，这种似然最大化方法可能导致退化，这指的是生成的输出是平淡、不连贯或陷入重复循环的。同时，人们发现NLG模型经常生成与提供的源输入不一致或不忠实的文本[85, 153, 159, 190]。研究人员开始将这种不良的生成称为幻觉[125]。

Hallucination in NLG is concerning because it hinders performance and raises safety concerns for real-world applications. For instance, in medical applications, a hallucinatory summary generated from a patient information form could pose a risk to the patient. It may provoke a life-threatening incident for a patient if the instructions of a medicine generated by machine translation are hallucinatory. Hallucination can also lead to potential privacy violations. Carlini et al. [20] demonstrate that language models can be prompted to recover and generate sensitive personal information from the training corpus (e.g., email address, phone/fax number, and physical address). Such memorization and recovery of the training corpus is considered a form of hallucination because the model is generating text that is not “faithful” to the source input content (i.e., such private information does not exist in the source input).

NLG中的幻觉令人担忧，因为它阻碍了性能，并引发了对现实世界应用的安全问题。例如，在医疗应用中，从患者信息表生成的幻觉摘要可能对患者构成风险。如果由机器翻译生成的药物说明是幻觉的，可能会引发患者的生命危险事件。幻觉也可能导致潜在的隐私侵犯。Carlini等人[20]证明，语言模型可以被提示从训练语料库中恢复和生成敏感的个人信息（例如，电子邮件地址、电话/传真号码和物理地址）。这种对训练语料库的记忆和恢复被认为是一种幻觉的形式，因为模型生成的文本不是“忠实于”源输入内容（即，这种私人信息不存在于源输入中）。

Currently there are many active efforts to address hallucination for various NLG tasks. Analyzing hallucinatory content in different NLG tasks and investigating their relationship would strengthen our understanding of this phenomenon and encourage the unification of efforts from different NLG fields. However, to date, little has been done to understand hallucinations from a broader perspective that encompasses all major NLG tasks. To the best of our knowledge, existing surveys have only focused specific tasks like abstractive summarization [76, 125] and translation [95]. Thus, in this paper, we present a survey of the research progress and challenges in the hallucination problem in NLG. And offer a comprehensive analysis of existing research on the phenomenon of hallucination in different NLG tasks, namely abstractive summarization, dialogue generation, generative question answering, data-to-text generation, machine translation. We mainly discussed hallucination of the unimodal NLG tasks that have textual input sources upon which the generated text can be assessed. We also briefly summarize hallucinations in multi-modal settings such as visual-language tasks [1, 13]. This survey can provide researchers a high-level insight derived from the similarities and differences of different approaches. Furthermore, given the various stages of development in studying hallucination from different tasks, the survey can assist researchers in drawing inspiration on concepts, metrics, and mitigation methods.

目前，有许多积极的努力来解决各种NLG任务中的幻觉问题。分析不同NLG任务中的幻觉内容，并研究它们之间的关系，将加强我们对这一现象的理解，并鼓励不同NLG领域的努力统一起来。然而，到目前为止，很少有工作从一个涵盖所有主要NLG任务的更广泛的角度来理解幻觉。据我们所知，现有的调查只关注了特定的任务，如抽象摘要[76, 125]和翻译[95]。因此，在本文中，我们对NLG中幻觉问题的研究进展和挑战进行了一项调查，并对不同NLG任务中幻觉现象的现有研究进行了全面的分析，即抽象摘要、对话生成、生成式问答、数据到文本生成、机器翻译。我们主要讨论了单模态NLG任务中的幻觉，这些任务具有文本输入源，可以根据这些输入源评估生成的文本。我们还简要总结了多模态设置下的幻觉，如视觉-语言任务[1, 13]。这项调查可以为研究人员提供一个高层次的洞察力，这些洞察力来自于不同方法之间的相似性和差异性。此外，考虑到从不同任务中研究幻觉的各个阶段，该调查可以帮助研究人员在概念、指标和缓解方法方面获得灵感。 

## 2 DEFINITIONS

