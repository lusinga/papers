# Language Models are Few-Shot Learners

Recent work has demonstrated substantial gains on many NLP tasks and benchmarks by pre-training on a large corpus of text followed by fine-tuning on a specific task. While typically task-agnostic in architecture, this method still requires task-specific fine-tuning datasets of thousands or tens of thousands of examples. By contrast, humans can generally perform a new language task from only a few examples or from simple instructions – something which current NLP systems still largely struggle to do. Here we show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art finetuning approaches. Specifically, we train GPT-3, an autoregressive language model with 175 billion parameters, 10x more than any previous non-sparse language model, and test its performance in the few-shot setting. For all tasks, GPT-3 is applied without any gradient updates or fine-tuning, with tasks and few-shot demonstrations specified purely via text interaction with the model. GPT-3 achieves strong performance on many NLP datasets, including translation, question-answering, and cloze tasks, as well as several tasks that require on-the-fly reasoning or domain adaptation, such as unscrambling words, using a novel word in a sentence, or performing 3-digit arithmetic. At the same time, we also identify some datasets where GPT-3’s few-shot learning still struggles, as well as some datasets where GPT-3 faces methodological issues related to training on large web corpora. Finally, we find that GPT-3 can generate samples of news articles which human evaluators have difficulty distinguishing from articles written by humans. We discuss broader societal impacts of this finding and of GPT-3 in general.

最近的研究表明，在大量文本语料库上进行预训练，然后针对特定任务进行微调，可以在许多NLP任务和基准测试中取得显著进步。尽管这种方法在架构上通常是任务不可知的，但仍需要数千或数万个特定任务的微调数据集。相比之下，人类通常只需通过几个示例或简单的指令就能完成新的语言任务，而当前的NLP系统在这方面仍然存在很大困难。在这里，我们展示了通过扩大语言模型的规模可以大幅提高任务不可知的少样本性能，有时甚至可以达到与先前最先进的微调方法相竞争的水平。具体而言，我们训练了GPT-3，一个拥有1750亿参数的自回归语言模型，比以前的任何非稀疏语言模型多10倍，并在少样本设定下测试其性能。对于所有任务，GPT-3在没有任何梯度更新或微调的情况下应用，任务和少样本演示仅通过与模型的文本交互来指定。GPT-3在许多NLP数据集上取得了强大的性能，包括翻译、问答和填空任务，以及一些需要即时推理或领域适应的任务，如重新排列单词、在句子中使用新词或执行3位数算术运算。同时，我们还发现GPT-3在某些数据集上的少样本学习仍然存在困难，以及在某些数据集上，GPT-3在处理大型网络语料库的训练方面存在方法论问题。最后，我们发现GPT-3可以生成新闻文章样本，人类评估者很难区分这些文章是由人类撰写的。我们讨论了这一发现以及GPT-3总体的更广泛的社会影响。
