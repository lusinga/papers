# Exploring Neuromorphic Computing Based on Spiking Neural Networks: Algorithms to Hardware

Neuromorphic Computing, a concept pioneered in the late 1980s, is receiving a lot of attention lately due to its promise of reducing the computational energy, latency, as well as learning complexity in artificial neural networks. Taking inspiration from neuroscience, this interdisciplinary field performs a multi-stack optimization across devices, circuits, and algorithms by providing an end-to-end approach to achieving brain-like efficiency in machine intelligence. On one side, neuromorphic computing introduces a new algorithmic paradigm, known as Spiking Neural Networks (SNNs), which is a significant shift from standard deep learning and transmits information as spikes (“1” or “0”) rather than analog values. This has opened up novel algorithmic research directions to formulate methods to represent data in spike-trains, develop neuron models that can process information over time, design learning algorithms for event-driven dynamical systems, and engineer network architectures amenable to sparse, asynchronous, event-driven computing to achieve lower power consumption. On the other side, a parallel research thrust focuses on development of efficient computing platforms for new algorithms. Standard accelerators that are amenable to deep learning workloads are not particularly suitable to handle processing across multiple timesteps efficiently. To that effect, researchers have designed neuromorphic hardware that rely on event-driven sparse computations as well as efficient matrix operations. While most large-scale neuromorphic systems have been explored based on CMOS technology, recently, Non-Volatile Memory (NVM) technologies show promise toward implementing bio-mimetic functionalities on single devices. In this article, we outline several strides that neuromorphic computing based on spiking neural networks (SNNs) has taken over the recent past, and we present our outlook on the challenges that this field needs to overcome to make the bio-plausibility route a successful one.

神经形态计算是一种在20世纪80年代末首创的概念，最近因其在人工神经网络中降低计算能量、延迟以及学习复杂性的承诺而受到广泛关注。这个跨学科领域从神经科学中汲取灵感，通过提供一种端到端的方法，在设备、电路和算法之间实现多层次优化，以实现类似大脑的机器智能效率。在一方面，神经形态计算引入了一种名为脉冲神经网络（SNNs）的新算法范式，它与标准深度学习有很大不同，并以脉冲（“1”或“0”）而非模拟值传输信息。这为新的算法研究方向提供了机会，以制定表示脉冲列车数据的方法、开发能够处理时间序列信息的神经元模型、设计面向事件驱动动态系统的学习算法以及构建适用于稀疏、异步、事件驱动计算的网络架构，以实现更低的功耗。另一方面，一个并行的研究方向关注于开发高效计算平台以适应新算法。适用于深度学习工作负载的标准加速器并不特别适合高效地处理跨多个时间步的处理。为此，研究人员设计了依赖于事件驱动稀疏计算以及高效矩阵运算的神经形态硬件。尽管大多数大规模神经形态系统都是基于CMOS技术进行探索的，但最近，非易失性存储器（NVM）技术在实现单一设备上的生物模拟功能方面显示出前景。在本文中，我们概述了基于脉冲神经网络（SNNs）的神经形态计算在最近的过去取得的几项重要进展，并展望了该领域需要克服的挑战，以使生物可信度路线取得成功。
